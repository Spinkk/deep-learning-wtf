{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fbbdbe",
   "metadata": {},
   "source": [
    "# Your first TensorFlow model\n",
    "\n",
    "For this exercise you will train a fully connected model to learn to identify handwritten digits (the MNIST data set). To do this I provide you with a function that prepares the data for you. Do not mind the warnings, they do not hint at any errors.\n",
    "\n",
    "## (1A) Write a fully connected layer by subclassing tf.Module.\n",
    "\n",
    "The layer should take two arguments, the number of inputs and the number of units. It should instantiate weights and biases in its constructor method (init) accordingly as a ``tf.Variable``.\n",
    "\n",
    "## (1B) Write a model by subclassing tf.keras.Model\n",
    "\n",
    "The model should use the fully connected layers that you defined before. It therefore needs the input shape as an argument in its constructor. \n",
    "\n",
    "The model should have one ``tf.keras.layers.Flatten()`` layer, followed by a single fully connected layer with ``10 units``, which has ``784 inputs`` since an mnist digit has shape (28,28). After this, we want to apply an activation function that turns the linear output into a categorical probability distribution over the 10 digit classes. To do so, we can use the softmax function. \n",
    "\n",
    "A softmax function normalizes an input along a particular (or multiple) axes to sum to 1 in a non-linear way such that small differences matter disproportionately less in the presence of large differences in value.\n",
    "\n",
    "$$\\text{softmax}(x) = \\frac{e^{x}}{\\sum_j^k e^{x_j}}$$\n",
    "\n",
    "The function can be instantiated as a layer with ``tf.keras.layers.Softmax(axis=-1)``.\n",
    "\n",
    "In addition to this, you should add a loss function as an attribute. For 10-way classification we use Categorical Crossentropy ``tf.keras.losses.CategoricalCrossentropy()``.\n",
    "\n",
    "The ``call method`` should output the softmaxed model output - not yet the loss, we will do this in the training loop!\n",
    "\n",
    "## (1C) Write a training_loop function \n",
    "\n",
    "Arguments: ``(model, train_ds, val_ds, epochs, learning_rate)``\n",
    "\n",
    "Returns: ``loss_history, val_loss_history``\n",
    "\n",
    "For a number of epochs we want to iterate over the mnist data set. During each iteration, we iterate over the entire data set, which returns a tuple ``(inputs, targets)``. Within the ``tf.GradientTape`` context we compute the forward pass and the loss of the model, and then use the tape to get the gradients of the loss with respect to ``model.trainable_variables``.\n",
    "The optimizer which applies the gradients to the variables is already implemented as a function for you. The training loop should call this function once you have the gradients. Once that is done, you append the loss value to ``loss_history``.\n",
    "\n",
    "After each epoch, you want to evaluate your model on data not trained on, this is why we have ``val_ds``. Like with ``train_ds`` you iterate over it and compute the loss, except here you do not use a gradient tape and you do not use an optimizer. The validation loss should be appended to `val_loss_history`.\n",
    "\n",
    "## (1D) Train the model for a few epochs and plot the (val) loss history with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dff5843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MathisPink\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MathisPink\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def get_mnist(batch_size):\n",
    "    \"\"\"\n",
    "    Load and prepare MNIST as a tensorflow dataset.\n",
    "    Returns a train and a validation dataset.\n",
    "\n",
    "    Args:\n",
    "    batch_size (int)\n",
    "    \"\"\"\n",
    "    # load data from tensorflow-datasets\n",
    "    train_ds, val_ds = tfds.load('mnist', split=['train', 'test'], shuffle_files=True)\n",
    "    \n",
    "    # function for one-hot encoding labels\n",
    "    one_hot = lambda x: tf.one_hot(x, 10)\n",
    "    \n",
    "    # function to make sure shapes are correct, apply one_hot to labels and normalize inputs\n",
    "    map_func = lambda x,y: (tf.cast(\n",
    "        tf.expand_dims(x, -1), dtype=tf.float32)/255.,\n",
    "                            tf.cast(one_hot(y),tf.float32))\n",
    "    \n",
    "    # turn dictionary data set into tuple data set (inputs, labels)\n",
    "    map_func_2 = lambda x: (x[\"image\"],x[\"label\"])\n",
    "    \n",
    "    # map the defined functions to the data set (they will be applied to each element while the data is loaded)\n",
    "    train_ds = train_ds.map(map_func_2).map(map_func)\n",
    "    val_ds   = val_ds.map(map_func_2).map(map_func)\n",
    "    \n",
    "    # shuffle, then create batches, then prefetch (preparing a batch while the model still computes the previous one)\n",
    "    train_ds = train_ds.shuffle(4096).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = val_ds.shuffle(4096).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return (train_ds, val_ds)\n",
    "\n",
    "train_ds, val_ds = get_mnist(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7e0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_step(variables, grads, learning_rate=1e-3):\n",
    "    for layer, gradient in zip(variables, grads):\n",
    "        for param, grad in zip(layer,gradient):\n",
    "            param.assign_sub(grad*learning_rate)\n",
    "\n",
    "\n",
    "class LinearLayer(tf.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        \n",
    "    def __call__(x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        \n",
    "        \n",
    "class DigitClassifier(tf.keras.Model):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    def call(x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "def training_loop(model, train_ds, val_ds, epochs, learning_rate=1e-3):\n",
    "    loss_history=[]\n",
    "    val_loss_history=[]\n",
    "    for e in range(epochs):\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
