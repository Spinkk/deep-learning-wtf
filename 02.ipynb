{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d00fbc",
   "metadata": {},
   "source": [
    "# Learning in ANNs: Gradient Descent, Backpropagation, Automatic Differentiation\n",
    "\n",
    "In this session, we will discuss how to train Multi-Layer Perceptrons (MLPs). The principles conveyed in this session are applicable not only to neural networks of different kinds but to any differentiable function. We will start from a historic perspective on ANNs and the difficulty of setting their weights and then discuss gradient descent and more generally reverse mode automatic differentiation. We will then go on to implement an ANN from scratch, derive local derivatives of involved computations and chain them to backpropagate gradients through the network.\n",
    "\n",
    "While this session is mathematically more involved, for practical purposes, getting the general idea will suffice. You will likely not need to do the math to compute gradients for your ANNs yourself. Knowing how they are computed however is very important to understand more involved phenomena and problems in optimization.\n",
    "\n",
    "### Historical background: The learning rule of a single perceptron (layer)\n",
    "\n",
    "A single perceptron (or a layer of perceptrons for multivariate targets) can be trained with a simple learning rule, the **Delta rule** that Frank Rosenblatt came up with already in 1958:\n",
    "$$\\Delta w_k^i = \\eta (y^i- {\\hat{y}}^i) x_k$$\n",
    "\n",
    "where $\\eta$ is a **learning rate**.\n",
    "\n",
    "Since a single layer of perceptrons can only discriminate between linearly separable feature vectors, there is a need to use multiple perceptron layers stacked on top of each other, using a sequence of (linear transformation, non-linearity). This limitation of perceptrons, and the lack of efficient weight update schemes for multi-layer perceptrons (and Rosenblatt's death before he could write a reply to criticism from symbolic AI proponent Marvin Minsky) effectively led to a first winter in ANN research. But now how do we update the weights in an MLP?\n",
    "\n",
    "To know how to update a weight, we need to know its contribution to the output of the MLP. How to do this efficiently was eventually figured out when David Rumelhart and Geoffrey Hinton came up with the idea of **error backpropagation** in 1985 while working on a research program they called _\"Parallel Distributed Processing\"_. A less known fact is that they were not the only ones who came up with this idea: Paul Werbos had already come up with the same scheme in his PhD thesis in 1974.\n",
    "\n",
    "The idea of backpropagation is to compute the gradients of the loss with respect to the weights. In the single layer case this could be done by the delta rule, so what the backpropagation represents is a **generalization of the delta rule** to the case where we have multiple layers of perceptrons.\n",
    "\n",
    "Later on, the idea of backpropagation was further generalized to yield algorithms for **automatic differentiation** on any function for which we know the derivatives of the individual local computations involved.\n",
    "\n",
    "Before diving into the idea of automatic differentiation, the aim of which is to have an efficient way of computing the gradients of the loss function with respect to the weights, we need to understand what _gradients of the loss function_ actually means and why finding these gradients would be useful for optimizing neural networks.\n",
    "\n",
    "The idea of gradient descent is that we can find minima of a function by following the opposite of its **gradient**. The gradient of a function of a vector of weights $\\vec{w}$ can be written as $\\nabla_{\\vec{w}} f(\\vec{w})$ and it is a vector of the partial derivatives of $f$ with respect to each element $w_i$ in $\\vec{w}$:\n",
    "\n",
    "$$\\nabla_{\\vec{w}} f(\\vec{w}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial w_0} \\\\\\frac{\\partial f}{\\partial w_1} \\\\... \\\\ \\frac{\\partial f}{\\partial w_n}\\end{pmatrix}$$\n",
    "\n",
    "This gradient points in the **locally steepest direction of function value increase**. If we want to minimize an error function, the idea is to follow the opposite direction of this gradient. By iteratively following and re-computing the gradient in **small steps**, we can end up reducing the error function. Given that $f$ is convex (meaning there exist no local minima that are not identical to the global minimum) and we take small enough steps, convergence to the global minimum is guaranteed.\n",
    "\n",
    "So we are interested in finding the partial derivatives of a loss/error function with respect to the weights. From calculus we know a bunch of rules that govern differentiation. Of particular interest to us is the **chain rule of calculus**: \n",
    "\n",
    "If we have a composite function $f(x) = g(h(x))$, then the chain rule can be expressed in shorthand notation as $$f'(x) = g'(h(x)) h'(x)$$\n",
    "\n",
    "Now with an MLP we do have such a composite function in the sense that one layer computes an output which then is the input to the next layer. In fact with two layers, we could define one layer as $g$ and the other layer as $h$ and yet another function $\\mathcal{L}$ as the error or **loss** function. Finding the gradients $\\nabla_w \\mathcal{L(w)}$ can be decomposed into finding partial derivatives of the loss function with respect to the parameters in layers $g$ and $h$.\n",
    "\n",
    "The idea is that we can first compute the partial derivatives of the outermost functions/layers and then, to obtain the derivatives of the earlier layers, we can multiply the local derivatives of their output with what we already computed for the outer functions. We have  $f = \\mathcal{L}(g(w_1, h(w_0, x)))$ and we want $f_{w_1}'$. We need to first differentiate through the loss function.\n",
    "\n",
    "For the mean squared error $\\mathcal{L}_{\\text{MSE}} = \\frac{1}{2} (\\hat{y}_w - y)^2$ this equals $\\hat{y}_w - y$. To then obtain the gradients w.r.t the weights $w_1$ of the outermost layer $g$, we multiply $\\hat{y}_w - y$ with $g'_w(x)$. The same logic applies to the weights of layer $h$. Once we have the partial derivatives w.r.t. the next layer's weights, we can compute the derivatives w.r.t. the previous layer's weights etc.. It is for this logic of computing gradients and then iteratively passing them down from the output of the ANN all the way to the input of the ANN (applying the chain rule) that it was called backpropagation.\n",
    "\n",
    "As it turns out, to compute the gradients in this fashion, it is highly useful to keep track of intermediate results during the **forward pass** (calling the MLP on an input), as they are needed to compute the local derivatives. The general logic of training MLPs thus is to first compute a prediction with a model and keep intermediate outputs in memory before using these values in the backward pass.\n",
    "\n",
    "## Automatic Differentiation\n",
    "\n",
    "While the original backpropagation learning rule was special to computations occuring in multi-layer perceptrons, a more modern approach is to have a library that tracks the **computational graph** and for each basic operation knows the derivative. During the backward pass the computational graph is reversely followed, making use of our knowledge of the derivatives of the individual small computations. With an automatic differentiation framework such as Tensorflow we can compute gradients (and thus perform gradient descent based optimization) on arbitrary functions that do not even have to be considered neural networks.\n",
    "\n",
    "Now that we've covered the general idea of how to update the weights of a neural network by incrementally following the negative of the gradient of the loss with respect to the model parameters, let's get our hands dirty by deriving some math and then applying the math to optimize a neural network that predicts real world housing prices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8dc60",
   "metadata": {},
   "source": [
    "# Training a neural network on the boston housing data\n",
    "\n",
    "The boston housing data is a real data set of 404 examples of houses and their prices. We want to use the 13 features to predict the price of a house. \n",
    "\n",
    "We create a neural network (multi-layer perceptron) that takes input data and transforms it, using linear transformations (matrix multiplication) and non-linear activation functions (sigmoid), to compute predictions of house price.\n",
    "\n",
    "We will use 3 layers. The first layer has 20 hidden units, the second layer has 10 hidden units and the third layer, which is the output layer, has 1 unit (just like the target). We use TensorFlow instead of numpy but really we use it in pretty much the same way as numpy. TensorFlow more conveniently allows to compute multiple matrix, matrix multiplications simultaneously compared to numpy, and it can run the code on a GPU and compile it for greater speed.\n",
    "\n",
    "The weight matrices have shape (13,20), (20,10) and (10,1), following the convention (n_in, n_out).\n",
    "\n",
    "We start by loading the training and validation data and defining a batch size. The batch size tells us how many examples we use to compute the gradients. The input data should have the shape (batch_size, n_features) and the labels should have the shape (batch_size, n_targets), where in this case n_targets is 1 because the house price that we predict is a scalar.\n",
    "\n",
    "# Ethics of predictive modeling\n",
    "\n",
    "When dealing with and learning about technical details of powerful tools such as machine learning, it is easy to forget about the real world and societal impact of these tools. Before attempting any project that involves real world data involving humans in machine learning or deep learning, there are some very important ethical questions that you should reflect on to evaluate the potential impact of your model/application. While models at initialization do usually not include any racial or other unwanted biases, the data usually reflects these biases. You may think that if the data does not contain a variable called \"race\", there is no risk of the model effectively using it to make decisions. This is unfortunately not true. There are almost always features that are caused and thereby correlated with features that we do not want to use to discriminate against people directly or indirectly (e.g. race, gender, ideology, political belief etc.)\n",
    "\n",
    "The particular dataset that we use here has been shown to have an ethical problem: the authors included a variable, \"B\", that may appear to assume that racial self-segregation influences house prices. The data set should thus not be used for any use case or analysis, unless in the context of illustrating ethical issues in data science and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "9e9fac92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled examples: 404\n",
      "Number of features used to predict house price: 13\n",
      "Number of features to predict: 1 \n",
      "\n",
      "batch size is 32\n",
      "inputs have shape (32, 13)\n",
      "targets have shape (32, 1)\n",
      "\n",
      "Example data: [0.01385168 0.         0.2934391  0.         0.61768085 0.70395416\n",
      " 0.917      0.37131545 0.16666667 0.4317862  0.95454544 1.\n",
      " 0.49302077]\n",
      "\n",
      "Example label: [15.2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size=32\n",
    "train_data, val_data = tf.keras.datasets.boston_housing.load_data()\n",
    "\n",
    "print(f\"Number of labeled examples: {train_data[0].shape[0]}\")\n",
    "print(f\"Number of features used to predict house price: {train_data[0].shape[1]}\")\n",
    "print(f\"Number of features to predict: 1 \\n\")\n",
    "print(f\"batch size is {batch_size}\")\n",
    "\n",
    "all_inputs = tf.constant(train_data[0], dtype=tf.float32)\n",
    "all_inputs = tf.reshape(all_inputs, (all_inputs.shape[0], all_inputs.shape[1]))\n",
    "\n",
    "val_all_inputs = tf.constant(val_data[0], dtype=tf.float32)\n",
    "val_all_inputs = tf.reshape(val_all_inputs, (val_all_inputs.shape[0], val_all_inputs.shape[1]))\n",
    "\n",
    "# normalize input data with training data statistics\n",
    "all_inputs = all_inputs / tf.reduce_max(all_inputs, axis=-2, keepdims=True)\n",
    "val_inputs = val_all_inputs / tf.reduce_max(all_inputs, axis=-2, keepdims=True)\n",
    "\n",
    "all_targets = tf.constant(train_data[1], dtype=tf.float32)\n",
    "all_targets = tf.expand_dims(all_targets, axis=-1)\n",
    "\n",
    "val_all_targets = tf.constant(val_data[1], dtype=tf.float32)\n",
    "val_all_targets = tf.expand_dims(val_all_targets, axis=-1)\n",
    "\n",
    "# look at first example batch\n",
    "inputs = all_inputs[:batch_size]\n",
    "target = all_targets[:batch_size]\n",
    "\n",
    "print(f\"inputs have shape {inputs.shape}\")\n",
    "print(f\"targets have shape {target.shape}\\n\")\n",
    "\n",
    "print(f\"Example data: {inputs[0]}\\n\")\n",
    "print(f\"Example label: {target[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38363a23",
   "metadata": {},
   "source": [
    "### The forward computation of an ANN\n",
    "\n",
    "Next we define the computations in which these weights are used.\n",
    "\n",
    "We have 3 layers and we want to apply a sigmoid activation function after every layer.\n",
    "\n",
    "We also compute the error between the output and the target values using a **Mean Squared Error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "0533c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions are [0.99328536 0.99328405 0.993286   0.99328405 0.9932863  0.99328476\n",
      " 0.9932864  0.9932858  0.99328506 0.99328595 0.9932864  0.9932859\n",
      " 0.9932844  0.9932863  0.99328554 0.9932862  0.99328655 0.9932841\n",
      " 0.99328434 0.99328506 0.9932858  0.9932841  0.9932846  0.9932838\n",
      " 0.9932845  0.99328643 0.9932847  0.99328405 0.9932852  0.993285\n",
      " 0.99328476 0.9932864 ] \n",
      " while the targets are [15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4 12.1 17.9 23.1 19.9\n",
      " 15.7  8.8 50.  22.5 24.1 27.5 10.9 30.8 32.9 24.  18.5 13.3 22.9 34.7\n",
      " 16.6 17.5 22.3 16.1]\n",
      "\n",
      " This makes a mean squared error of 272.683837890625\n"
     ]
    }
   ],
   "source": [
    "weights_1 = tf.random.uniform(shape=(13,20))\n",
    "weights_2 = tf.random.uniform(shape=(20,10))\n",
    "weights_3 = tf.random.uniform(shape=(10,1))\n",
    "\n",
    "# layer 1\n",
    "f7_o = inputs @ weights_1\n",
    "# activation function of layer 1\n",
    "f6_o = tf.nn.sigmoid(f7_o)\n",
    "\n",
    "# layer 2\n",
    "f5_o = f6_o @ weights_2\n",
    "# activation function of layer 2\n",
    "f4_o = tf.nn.sigmoid(f5_o)\n",
    "\n",
    "# layer 3 (output layer)\n",
    "f3_o = f4_o @ weights_3\n",
    "# activation function of layer 3 (output layer)\n",
    "f2_o = tf.nn.sigmoid(f3_o)\n",
    "\n",
    "# error function, here MSE\n",
    "f1_o = tf.reduce_mean(0.5 * (f2_o - target)**2, axis=None) # mean squared error\n",
    "\n",
    "print(f\"predictions are {f2_o.numpy().squeeze()} \\n while the targets are {target.numpy().squeeze()}\")\n",
    "print(f\"\\n This makes a mean squared error of {f1_o}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cfac07",
   "metadata": {},
   "source": [
    "Our model can be described as a composite function of 7 computations, most of which are already composite functions (e.g. sigmoid activation function or the MSE-loss):\n",
    "\n",
    "$$\\mathcal{L} = f_1(f_2(f_3( f_4(f_5(f_6(f_7(x,w_1)), w_2  )  ) ) , w_3)   , \\text{target})$$\n",
    "\n",
    "What we want to find is the tuple of partial derivatives, that is the gradient, of $f_1$ w.r.t. the parameters ($w_1, w_2, w_3$).\n",
    "\n",
    "#### Using the chain rule\n",
    "\n",
    "Because this is a composite function we can apply the **chain rule** of calculus, according to which the derivative is computed as the outer derivative given the unchanged output of the inner function, multiplied by the derivative of the inner function, again keeping its input unchanged, if $x$ in the formula below turns out to be another function:\n",
    "\n",
    "$$\\frac{d}{{dx}}\\left[ {f_1\\left( f_2 \\right)} \\right] = \\frac{d}{{df_2}}\\left[ {f_1\\left( f_2 \\right)} \\right]\\frac{{df_2}}{{dx}}$$\n",
    "\n",
    "\n",
    "### Local derivatives\n",
    "\n",
    "All we need to compute the partial derivatives of the loss function w.r.t. the network parameters (here only the weights, since we do not use a bias) is the derivatives of the individual or composite computations involved.\n",
    "\n",
    "Our composite function (the joint forward pass and loss computation of an ANN) makes use of three kinds of composite computations:\n",
    "- Mean squared error (MSE)\n",
    "- Sigmoid activation function\n",
    "- Matrix multiplication\n",
    "\n",
    "### Local derivatives of MSE\n",
    "\n",
    "$\\mathcal{L}_{\\text{MSE}} = f_1(\\text{f2_o}) = \\frac{1}{2} (\\text{f2_o} - \\text{t})^2$\n",
    "\n",
    "The derivative of this w.r.t. the ANN's final layer's output *f2_o* is\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\text{f2_o}} = \\text{f2_o} - \\text{t}$\n",
    "\n",
    "### Local derivatives of the sigmoid activation function\n",
    "\n",
    "Activation functions are typically denoted as $\\sigma(x)$. Here, the sigmoid activation function is defined by:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1+\\text{e}^{-x}}$\n",
    "\n",
    "and its derivative turns out to be\n",
    "\n",
    "$\\frac{\\partial \\sigma}{\\partial \\text{x}} = \\sigma(x) ( 1 - \\sigma(x))$\n",
    "\n",
    "Since an activation function is an element-wise operation, we also chain this local derivative $\\sigma'(x)$ with $\\frac{\\partial \\mathcal{L}}{\\partial \\sigma(x)}$ by element-wise multiplication.\n",
    "\n",
    "### Local derivatives of matrix multiplication\n",
    "\n",
    "A matrix multiplication is a linear operation that takes two arguments, so we can differentiate w.r.t. either the first matrix or the second matrix - in neural networks this is either the input to the layer or the weights of that layer.\n",
    "\n",
    "Let's say we have the following matrix multiplication\n",
    "\n",
    "$C = A B$\n",
    "\n",
    "and we already have the gradients of the loss function w.r.t. the output of this matrix multiplication C, that is we have $\\frac{\\partial \\mathcal{L}}{\\partial C}$.\n",
    "\n",
    "The partial derivative of the loss $\\mathcal{L}$ w.r.t. $A$ turns out to be\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial A} = \\frac{\\partial \\mathcal{L}}{\\partial C} B^{\\text{T}}$.\n",
    "\n",
    "Likewise the partial derivative w.r.t. matrix $B$ turns out to be\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial B} = A^{\\text{T}} \\frac{\\partial \\mathcal{L}}{\\partial C}$.\n",
    "\n",
    "**For a handwritten explanation, please refer to the additional material provided in the same folder as this notebook.**\n",
    "\n",
    "\n",
    "This is it, we know all the local derivatives involved in the computation of the loss that we want to minimize by changing the weights in the three layers. \n",
    "\n",
    "Let's have another look at the forward computation of the loss, in which we store intermediate computations and below refer to them in the math notation of the backpropagation by their variable names, hopefully making it easier to relate the math to the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5067468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1\n",
    "f7_o = tf.matmul(inputs, weights_1)\n",
    "# activation function of layer 1\n",
    "f6_o = tf.nn.sigmoid(f7_o)\n",
    "\n",
    "# layer 2\n",
    "f5_o = tf.matmul(f6_o, weights_2)\n",
    "# activation function of layer 2\n",
    "f4_o = tf.nn.sigmoid(f5_o)\n",
    "\n",
    "# layer 3 (output layer)\n",
    "f3_o = tf.matmul(f4_o, weights_3)\n",
    "# activation function of layer 3 (output layer)\n",
    "f2_o = tf.nn.sigmoid(f3_o)\n",
    "\n",
    "# error function, here MSE\n",
    "f1_o = tf.reduce_mean(0.5 * (f2_o - target)**2,axis=None) # mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c0c78",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "We start with the partial derivative of the mean squared error w.r.t. the network output f2_o:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_2} = \\text{f2_o} - \\text{t}$$\n",
    "\n",
    "Next we want to differentiate through the sigmoid activation function. We take an element-wise product between the outer derivative and the inner derivative $\\sigma'(x)$, the definition of which you can find in the *local derivatives* section.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_3} = \\frac{\\partial \\mathcal{L}}{\\partial f_2} \\circ \\sigma'(\\text{f3_o})$$\n",
    "\n",
    "We can already compute the gradients w.r.t. the weights of the last layer. We use the result we got above in the *local derivatives* section to know how to do this.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\text{W}_3} = \\text{f4_o}^{\\text{T}} \\frac{\\partial \\mathcal{L}}{\\partial f_3}$$\n",
    "\n",
    "Now we want to differentiate with respect to the last layer's input, f4_o - again using the logic from the *local derivatives* section.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_4} = \\frac{\\partial \\mathcal{L}}{\\partial f_3} {\\text{W}_3}^{\\text{T}}$$\n",
    "\n",
    "The next step to differentiate through backwards is again a sigmoid activation function. We can thus follow the same procedure as we did before:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_5} = \\frac{\\partial \\mathcal{L}}{\\partial f_4} \\circ \\sigma'(\\text{f5_o})$$\n",
    "\n",
    "Now we want to differentiate through the matrix multiplication of layer 2, first w.r.t. the layer's input:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_6} = \\frac{\\partial \\mathcal{L}}{\\partial f_5} {\\text{W}_3}^{\\text{T}}$$\n",
    "\n",
    "We can now get the gradients of the weights of layer 2, by differentiating the same matmul operation from the forward computation w.r.t. the weights, following the same logic as before for the gradients of weights_3:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\text{W}_2} = \\text{f6_o}^{\\text{T}} \\frac{\\partial \\mathcal{L}}{\\partial f_6}$$\n",
    "\n",
    "This is becoming quite repetitive - given that we have the gradients of the loss function w.r.t. the second layer's inputs, we can differentiate through the activation function of layer 1:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_7} = \\frac{\\partial \\mathcal{L}}{\\partial f_6} \\circ \\sigma'(\\text{f7_o})$$\n",
    "\n",
    "Sometimes we are interested in the gradients of the loss w.r.t. the input data. We can obtain these by differentiating through the matrix multiplication of the first layer w.r.t. the inputs:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\text{inputs}} = \\frac{\\partial \\mathcal{L}}{\\partial f_7}  {\\text{W}_1}^{\\text{T}}$$\n",
    "\n",
    "Finally, and lastly, we compute the gradients of the loss $\\mathcal{L}$ w.r.t. the first layer's weights:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\text{W}_1} = \\text{f7_o}^{\\text{T}} \\frac{\\partial \\mathcal{L}}{\\partial f_7}$$\n",
    "\n",
    "\n",
    "Now that we have derived the computations for chaining the local derivatives such that we compute the gradients of the mean-squared-error loss w.r.t. the weights, we can compute the gradients using TensorFlow (again, numpy would also work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "392a92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients of MSE (d L/ d f2)\n",
    "grad_f1_o = (f2_o - target)\n",
    "\n",
    "# Chain gradients of MSE with gradients of output activation function\n",
    "# (d L / d f3)\n",
    "grad_f2_o = grad_f1_o *(tf.nn.sigmoid(f3_o)*(1-tf.nn.sigmoid(f3_o)))\n",
    "\n",
    "# compute gradients w.r.t the last layer's weights (weights_3)\n",
    "# (d L / d weights_3)\n",
    "grad_w3 = (tf.transpose(f4_o) @ grad_f2_o) / batch_size\n",
    "\n",
    "# chain with derivative of matrix multiplication of layer 3\n",
    "# d L / d f4\n",
    "grad_f3_o = grad_f2_o @ tf.transpose(weights_3)\n",
    "\n",
    "# chain with derivative of activation function (layer 3)\n",
    "# d L / d f5\n",
    "grad_f4_o = grad_f3_o * (tf.nn.sigmoid(f5_o)*(1-tf.nn.sigmoid(f5_o)))\n",
    "\n",
    "# chain with derivative of matrix multiplication of layer 2\n",
    "# d L / d f6\n",
    "grad_f5_o = grad_f4_o @ tf.transpose(weights_2)\n",
    "\n",
    "# compute gradients w.r.t. layer 2's weight matrix\n",
    "# d L / d weights_2\n",
    "grad_w2 = (tf.transpose(f6_o) @ grad_f4_o) / batch_size\n",
    "\n",
    "# chain with layer 2's sigmoid activation function\n",
    "# d L / d f7\n",
    "grad_f6_o = grad_f5_o * (tf.nn.sigmoid(f7_o)*(1-tf.nn.sigmoid(f7_o)))\n",
    "\n",
    "# compute gradients w.r.t. the input\n",
    "# d L / d inputs\n",
    "grad_inputs = grad_f6_o @ tf.transpose(weights_1)\n",
    "\n",
    "# compute gradients w.r.t. layer 1's weights\n",
    "# d L / d weights_1\n",
    "grad_w1 = (tf.transpose(inputs) @ grad_f6_o) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c10b27",
   "metadata": {},
   "source": [
    "Let's check whether our implementation that we got by chaining local derivatives of atomic computations involved actually gives us the correct gradients w.r.t. to the network weights.\n",
    "\n",
    "To do this, we rely on Tensorflow's **reverse mode automatic differentiation**, which does exactly what we just did, relying on optimized computational graph structures to reduce the number of operations. In the next session (03) you will learn how to use this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "341d2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([weights_1, weights_2, weights_3])\n",
    "    f7_o = tf.matmul(inputs, weights_1)\n",
    "    f6_o = tf.nn.sigmoid(f7_o)\n",
    "    f5_o = tf.matmul(f6_o, weights_2)\n",
    "    f4_o = tf.nn.sigmoid(f5_o)\n",
    "    f3_o = tf.matmul(f4_o, weights_3)\n",
    "    f2_o = tf.nn.sigmoid(f3_o)\n",
    "\n",
    "    f1_o = tf.reduce_mean(0.5 * (f2_o - target)**2) # mean squared error\n",
    "    \n",
    "real_grad_w1, real_grad_w2, real_grad_w3 = tape.gradient(f1_o, [weights_1,\n",
    "                                                                weights_2,\n",
    "                                                                weights_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "1e5223ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients for weights_1 match tensorflow oracle:     True\n",
      "gradients for weights_2 match tensorflow oracle:     True\n",
      "gradients for weights_3 match tensorflow oracle:     True\n"
     ]
    }
   ],
   "source": [
    "tolerance = 0.0\n",
    "print(f\"gradients for weights_1 match tensorflow oracle: \\\n",
    "    {np.allclose(grad_w1, real_grad_w1, atol=tolerance)}\")\n",
    "print(f\"gradients for weights_2 match tensorflow oracle: \\\n",
    "    {np.allclose(grad_w2, real_grad_w2, atol=tolerance)}\")\n",
    "print(f\"gradients for weights_3 match tensorflow oracle: \\\n",
    "    {np.allclose(grad_w3, real_grad_w3, atol=tolerance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d0a40",
   "metadata": {},
   "source": [
    "Now that we have some confidence that both our math and our implementation are correct, we can use this to train the ANN on the boston housing data! For that we need to define the training logic.\n",
    "\n",
    "The procedure is as follows, we define a **learning rate** and a **batch size**, and then we iterate over batches of examples from the data set. Once we've processed every element in the training data, we're done with one **epoch** and we repeat the next process until we have trained for a desired number of epochs.\n",
    "\n",
    "Below there are some differences to the above network:\n",
    "\n",
    "- we initialize the weights differently (more on this in a later session)\n",
    "\n",
    "- we include a bias in our network (the local derivative of the bias is just 1)\n",
    "\n",
    "- we remove the sigmoid on the output, because it is a regression task\n",
    "\n",
    "To train the network weights, we need four functions:\n",
    "- the **forward computation** (including the loss function), that also returns the intermediate results\n",
    "- the **backward (gradient) computation** that takes the results from forward computation and backpropagates the gradients to the network parameters, returning a tuple of parameter gradients.\n",
    "- the **weight update**, in which we pair the gradients with their respective parameter array/tensor and subtract a fraction of the gradients from the parameters\n",
    "- the **training loop** which applies the three functions for a number of iterations, computing the loss (and intermediate activations and pre-activations), backpropagating the error (computing gradients), and applying the gradients to update the weights and the biases of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "4af121da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(n_inputs, n_units_1, n_units_2, n_units_3):\n",
    "\n",
    "    # He-Uniform initialization for better learning\n",
    "    limit_1 = np.sqrt(6 / n_inputs)\n",
    "    limit_2 = np.sqrt(6 / n_units_1)\n",
    "    limit_3 = np.sqrt(6 / n_units_2)\n",
    "\n",
    "    weights_1 = tf.random.uniform(minval=-limit_1, maxval=limit_1, shape=(n_inputs,n_units_1))\n",
    "    weights_2 = tf.random.uniform(minval=-limit_2, maxval=limit_2, shape=(n_units_1,n_units_2))\n",
    "    weights_3 = tf.random.uniform(minval=-limit_3, maxval=limit_3, shape=(n_units_2,n_units_3))\n",
    "\n",
    "    # this time using a bias in our model\n",
    "    bias_1 = tf.Variable(tf.zeros(n_units_1))\n",
    "    bias_2 = tf.Variable(tf.zeros(n_units_2))\n",
    "    bias_3 = tf.Variable(tf.zeros(n_units_3))\n",
    "\n",
    "    weights_1 = tf.Variable(weights_1)\n",
    "    weights_2 = tf.Variable(weights_2)\n",
    "    weights_3 = tf.Variable(weights_3)\n",
    "\n",
    "    weights = ((weights_1,bias_1), \n",
    "               (weights_2,bias_2), \n",
    "               (weights_3, bias_3))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "297a5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True) # for speed\n",
    "def compute_forward_pass(inputs, target, weights):\n",
    "    \n",
    "    weights_1 = weights[0][0]\n",
    "    weights_2 = weights[1][0]\n",
    "    weights_3 = weights[2][0]\n",
    "    bias_1    = weights[0][1]\n",
    "    bias_2    = weights[1][1]\n",
    "    bias_3    = weights[2][1]\n",
    "    \n",
    "    # layer 1, now with bias\n",
    "    f7_o = tf.matmul(inputs, weights_1) + bias_1\n",
    "    # activation function of layer 1\n",
    "    f6_o = tf.nn.sigmoid(f7_o)\n",
    "    \n",
    "    # layer 2, now with bias\n",
    "    f5_o = tf.matmul(f6_o, weights_2) + bias_2\n",
    "    # activation function of layer 2\n",
    "    f4_o = tf.nn.sigmoid(f5_o)\n",
    "\n",
    "    # layer 3 (output layer), now with bias\n",
    "    f3_o = tf.matmul(f4_o, weights_3) + bias_3\n",
    "    # activation function of layer 3 (output layer)\n",
    "    # skip the last layer's sigmoid f2_o = tf.nn.sigmoid(f3_o) \n",
    "\n",
    "    # error function, here MSE\n",
    "    f1_o = tf.reduce_mean(0.5 * (f3_o - target)**2) # mean squared error\n",
    "\n",
    "    return (f1_o, f3_o, f4_o, f5_o, f6_o, f7_o, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "33502edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True) # for speed\n",
    "def compute_gradients(weights, data_from_forward, target, batch_size):\n",
    "    \n",
    "    # unpack weights and data from the forward-step\n",
    "    ((weights_1,bias_1), \n",
    "     (weights_2, bias_2), \n",
    "     (weights_3,bias_3)) = weights\n",
    "    (f1_o, f3_o, f4_o, f5_o, f6_o, f7_o, inputs) = data_from_forward\n",
    "    \n",
    "    # Gradients of MSE (d L/ d f2)\n",
    "    grad_f1_o = (f3_o - target)\n",
    "\n",
    "    # chain with derivative of matrix multiplication of layer 3\n",
    "    # d L / d f4\n",
    "    grad_f3_o = grad_f1_o @ tf.transpose(weights_3)\n",
    "    \n",
    "    # compute gradients w.r.t the last layer's weights (weights_3)\n",
    "    # (d L / d weights_3)\n",
    "    grad_w3 = (tf.transpose(f4_o) @ grad_f1_o) / batch_size\n",
    "    \n",
    "    # (d L / d bias_3)\n",
    "    grad_b3 = tf.reduce_sum(grad_f1_o,axis=0) / batch_size\n",
    "\n",
    "    # chain with derivative of activation function (layer 3)\n",
    "    # d L / d f5\n",
    "    grad_f4_o = grad_f3_o * (tf.nn.sigmoid(f5_o)*(1-tf.nn.sigmoid(f5_o)))\n",
    "\n",
    "    # chain with derivative of matrix multiplication of layer 2\n",
    "    # d L / d f6\n",
    "    grad_f5_o = grad_f4_o @ tf.transpose(weights_2)\n",
    "\n",
    "    # compute gradients w.r.t. layer 2's weight matrix\n",
    "    # d L / d weights_2\n",
    "    grad_w2 = (tf.transpose(f6_o) @ grad_f4_o) / batch_size\n",
    "    \n",
    "    # (d L / d bias_2)\n",
    "    grad_b2 = tf.reduce_sum(grad_f4_o,axis=0) / batch_size\n",
    "\n",
    "    # chain with layer 2's sigmoid activation function\n",
    "    # d L / d f7\n",
    "    grad_f6_o = grad_f5_o * (tf.nn.sigmoid(f7_o)*(1-tf.nn.sigmoid(f7_o)))\n",
    "\n",
    "    # compute gradients w.r.t. the input\n",
    "    # d L / d inputs\n",
    "    grad_inputs = grad_f6_o @ tf.transpose(weights_1)\n",
    "\n",
    "    # compute gradients w.r.t. layer 1's weights\n",
    "    # d L / d weights_1\n",
    "    grad_w1 = (tf.transpose(inputs) @ grad_f6_o) / batch_size\n",
    "    \n",
    "    # (d L / d bias_1)\n",
    "    grad_b1 = tf.reduce_sum(grad_f6_o,axis=0) / batch_size\n",
    "\n",
    "    return ((grad_w1, grad_b1), \n",
    "            (grad_w2, grad_b2), \n",
    "            (grad_w3, grad_b3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "9b973190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights, weight_gradients, learning_rate=1e-2):\n",
    "    for params, grads in zip(weights, weight_gradients):\n",
    "        for p, p_grad in zip(params, grads):\n",
    "            p = p.assign(p - learning_rate*p_grad)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "e759d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_loss(weights, batch_size, \n",
    "                  learning_rate, epochs=2, \n",
    "                  training_data=None, \n",
    "                  val_data=None):\n",
    "    \n",
    "    all_inputs, all_targets = training_data\n",
    "    val_all_inputs, val_all_targets = val_data if val_data else (None,None)\n",
    "    epoch_wise_losses = []\n",
    "    val_epoch_wise_losses = []\n",
    "    \n",
    "    # iterate over epochs\n",
    "    for e in range(epochs):\n",
    "        losses = []\n",
    "        \n",
    "        # iterate over batches in the data set\n",
    "        for i in range(all_inputs.shape[0]//batch_size):\n",
    "            \n",
    "            # subset the training data for the current batch\n",
    "            inputs = all_inputs[i*batch_size:i*batch_size+batch_size]\n",
    "            targets = all_targets[i*batch_size:i*batch_size+batch_size]\n",
    "            \n",
    "            # do the forward pass\n",
    "            data_from_forward = compute_forward_pass(inputs, \n",
    "                                                     targets, \n",
    "                                                     weights)\n",
    "            # log the loss\n",
    "            losses.append(data_from_forward[0].numpy())\n",
    "            \n",
    "            # compute gradients\n",
    "            grads = compute_gradients(weights, data_from_forward, \n",
    "                                      targets, \n",
    "                                      batch_size=batch_size)\n",
    "            # update the weights\n",
    "            weights = update_weights(weights, grads, learning_rate=learning_rate)\n",
    "            \n",
    "        # report loss at end of epoch\n",
    "        print(f\"epoch:{e} loss={np.mean(losses)}\")\n",
    "        # log loss for epoch\n",
    "        epoch_wise_losses.append(np.mean(losses))\n",
    "        \n",
    "        if val_data:\n",
    "            val_data_from_forward = compute_forward_pass(val_all_inputs, \n",
    "                                                     val_all_targets, \n",
    "                                                     weights)\n",
    "            val_epoch_wise_losses.append(val_data_from_forward[0].numpy())\n",
    "            print(f\"epoch:{e} val_loss={val_epoch_wise_losses[-1]}\\n\")\n",
    "    return epoch_wise_losses, val_epoch_wise_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "2a4ac656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss=153.89581298828125\n",
      "epoch:0 val_loss=75.04292297363281\n",
      "\n",
      "epoch:1 loss=50.2238883972168\n",
      "epoch:1 val_loss=46.17360305786133\n",
      "\n",
      "epoch:2 loss=43.004207611083984\n",
      "epoch:2 val_loss=43.477569580078125\n",
      "\n",
      "epoch:3 loss=42.73936462402344\n",
      "epoch:3 val_loss=42.92287063598633\n",
      "\n",
      "epoch:4 loss=42.66010665893555\n",
      "epoch:4 val_loss=42.71744155883789\n",
      "\n",
      "epoch:5 loss=42.5479850769043\n",
      "epoch:5 val_loss=42.51738357543945\n",
      "\n",
      "epoch:6 loss=42.401912689208984\n",
      "epoch:6 val_loss=42.4486083984375\n",
      "\n",
      "epoch:7 loss=42.204463958740234\n",
      "epoch:7 val_loss=41.99867248535156\n",
      "\n",
      "epoch:8 loss=41.91493606567383\n",
      "epoch:8 val_loss=41.37021255493164\n",
      "\n",
      "epoch:9 loss=41.45710754394531\n",
      "epoch:9 val_loss=40.72182846069336\n",
      "\n",
      "epoch:10 loss=40.747459411621094\n",
      "epoch:10 val_loss=39.94678497314453\n",
      "\n",
      "epoch:11 loss=39.82044982910156\n",
      "epoch:11 val_loss=39.28278350830078\n",
      "\n",
      "epoch:12 loss=38.78224182128906\n",
      "epoch:12 val_loss=37.677764892578125\n",
      "\n",
      "epoch:13 loss=37.651248931884766\n",
      "epoch:13 val_loss=36.88502883911133\n",
      "\n",
      "epoch:14 loss=36.409461975097656\n",
      "epoch:14 val_loss=36.10092544555664\n",
      "\n",
      "epoch:15 loss=35.05064392089844\n",
      "epoch:15 val_loss=35.147743225097656\n",
      "\n",
      "epoch:16 loss=33.58803939819336\n",
      "epoch:16 val_loss=34.283870697021484\n",
      "\n",
      "epoch:17 loss=32.08635711669922\n",
      "epoch:17 val_loss=32.93873977661133\n",
      "\n",
      "epoch:18 loss=30.67186737060547\n",
      "epoch:18 val_loss=31.20799446105957\n",
      "\n",
      "epoch:19 loss=29.437894821166992\n",
      "epoch:19 val_loss=30.520965576171875\n",
      "\n",
      "epoch:20 loss=28.378049850463867\n",
      "epoch:20 val_loss=29.81673240661621\n",
      "\n",
      "epoch:21 loss=27.431062698364258\n",
      "epoch:21 val_loss=29.445514678955078\n",
      "\n",
      "epoch:22 loss=26.534822463989258\n",
      "epoch:22 val_loss=28.731178283691406\n",
      "\n",
      "epoch:23 loss=25.645326614379883\n",
      "epoch:23 val_loss=28.58278465270996\n",
      "\n",
      "epoch:24 loss=24.736427307128906\n",
      "epoch:24 val_loss=28.796709060668945\n",
      "\n",
      "\n",
      " For previously unseen houses, the model's predictions are off by 5.3662567138671875 on average\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 13\n",
    "n_units_1 = 16\n",
    "n_units_2 = 8\n",
    "n_units_3 = 1\n",
    "\n",
    "# instantiate weights\n",
    "weights = initialize_weights(n_inputs, n_units_1, n_units_2, n_units_3)\n",
    "\n",
    "training_losses, val_losses = minimize_loss(weights=weights,\n",
    "                      batch_size=8, epochs=25,\n",
    "                      learning_rate=4e-3, training_data=(all_inputs, all_targets),\n",
    "                                           val_data = (val_all_inputs, val_all_targets))\n",
    "\n",
    "print(f\"\\n For previously unseen houses, the model's predictions are off by {np.sqrt(val_losses[-1])} on average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "6392d767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses, label=\"train\")\n",
    "plt.plot(val_losses,label=\"val\")\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb76f3",
   "metadata": {},
   "source": [
    "We have just built our first artificial neural network almost completely from scratch! First we derived the computation of gradients mathematically by using the chain rule and subsequently implemented this in TensorFlow, which allowed us to easily perform highly optimized batched matrix multiplications. We evaluated our solution against TensorFlow's automatic differentiation and found that the computed gradients match to the last floating point. Lastly we implemented a complete training logic to train the network architecture on the ethically questionable Boston Housing data set, a real world data set with 404 examples of houses, described by 13 features, to predict their prices.\n",
    "\n",
    "\n",
    "## Some (important) notes\n",
    "\n",
    "You may encounter some unanswered questions when trying to find gradients for more complicated composite functions. One possibility is that the same weight matrix (or activation tensor) affects the loss through multiple paths. In this case we need to aggregate the gradients obtained through these paths by summation. The same applies for a case in which we have a batch of inputs and not a single (input, output) example. In this case, you also aggregate the gradients obtained for each example by averaging, as we've done above. Deep Learning frameworks do batched matrix multiplications in a highly optimized way by utilizing the capabilities for parallelism of modern graphic processing units (GPUs) or other accelerators.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have found the computations needed to obtain the gradients of our composite function w.r.t. some variables (here weights and biases). Let's note some important observations that we may have made along the way:\n",
    "\n",
    "- Ethical considerations on whether data implicitly contains historically grounded racial or other biases are important and should come first, not as an afterthought. The data set we used here for illustration does contain such biases and should not be taken for any serious analysis or modeling.\n",
    "\n",
    "\n",
    "- Doing it by hand is tedious and the mathematical expressions for the gradients get very long, even for a shallow neural network of just 3 layers.\n",
    "\n",
    "\n",
    "- We really only need to know the local derivatives of the basic computations involved in a composite function (such as any kind of ANN) to compute gradients of the complete function.\n",
    "\n",
    "\n",
    "- The intermediate computations that arise in the function's forward pass are re-used in the backward computation of its gradients.\n",
    "\n",
    "\n",
    "- The same logic can be applied for any kind of directed acyclic graph composed of smaller functions that are differentiable. As such, it can be used to compute gradients for any neural network architecture (CNNs, RNNs, etc.), as long as the individual network components have known local derivatives.\n",
    "\n",
    "\n",
    "- In automatic differentiation frameworks, functions are constructed on graphs that contain inputs, parameters, as well as the operations, their local derivatives and the intermediate results to be re-used in the backward pass. There is no need to chain derivatives together manually in these frameworks as we did here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
