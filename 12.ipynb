{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a0bcbd",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "In this session we have a brief look at the architecture of autoencoders and why they can be useful. You will learn about the concept of encoders and decoders, the idea of representation learning and dimensionality reduction.\n",
    "\n",
    "The prefix \"auto\" means \"self\". What autoencoders do is take an input and learn to encode it for the sake of being able to reconstruct that input. This may sound pretty useless at first. However with autoencoders we are usually not interested in the model's output - the reconstruction. That can be useful if we want to eliminate artifacts in our raw data but generally we are interested in a hidden representation within the autoencoder. To understand why, we need to take a look at the architecture first.\n",
    "\n",
    "![https://upload.wikimedia.org/wikipedia/commons/3/37/Autoencoder_schema.png](https://upload.wikimedia.org/wikipedia/commons/3/37/Autoencoder_schema.png)\n",
    "\n",
    "The encoder takes a very high dimensional input $x$ (either an image or a feature vector) and it reduces the dimensionality such that we obtain a lower dimensional latent representation $x_h$. The encoding of $x$ is trained such that a decoder can reconstruct the original input $x$ from $x_h$. This means that $x_h$ needs to be maximally informative of $x$ and thus contain all relevant information about it in a compact representation.\n",
    "\n",
    "Why do we care about compact representations of data with reduced dimensionality? Reducing the dimensionality of data can serve multiple purposes. For learning ANNs it can help models generalize better and be computationally more efficient because there are fewer parameters and computations needed to process the low-dimensional representation compared to the high-dimensional raw data. For images, in CNNs, this takes the form of an initial \"stem\" part of models that down-sample the size of the image by employing large kernel sizes with striding or pooling (see session 08). Other times, a low dimensional representation of high dimensional data is desired because it allows to visualize the data. \n",
    "\n",
    "The simplest autoencoder is a linear autoencoder, without any non-linearities. You can think of it as a weight matrix $W$ that projects a vector $x$ into a lower dimensional vector space, such that we obtain $x_h$. Another matrix then maps back to the high dimensional state.\n",
    "\n",
    "$x_h = \\text{W}_{\\text{encoder}} x$\n",
    "\n",
    "$\\hat{x} = \\text{W}_{\\text{decoder}} x_h $\n",
    "\n",
    "Such that $ \\mathcal{L}_{\\text{MSE}} = \\frac{1}{n}\\sum_i^n ({\\hat{x}}_i - x_i)^2$ is minimized.\n",
    "\n",
    "The solution to this is equivalent to a projection onto the first $h$ **principal components** of $X$ if $h$ is the dimensionality of the encoding. We can determine the weight matrix $\\text{W}_{\\text{encoder}}$ by computing the eigenvectors of the data's covariance matrix associated with the highest eigenvalues. The eigenvectors of the covariance matrix (which we can compute with singular value decomposition) are the principal components of the data and they point in directions in which the variance is highest while being orthogonal to each other.\n",
    "\n",
    "We can consider autoencoder architectures in deep learning to be doing principal component analysis without a closed-form analytical solution but instead relying on gradient descent and explicit reconstruction. If we employ more complex autoencoders, including multiple layers and non-linearities, we enter the realm of non-linear dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9807bdd",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoders (transposed convolutions)\n",
    "\n",
    "To down-sample images you can use convolutional layers either without padding, by using pooling, or by using strides larger than 1. We can also do up-sampling with convolutional layers. A variant of a convolutional operation that does that is called transposed convolution. In a transposed convolution, each location in the input feature map is multiplied by each element in the kernel and then written to a larger output feature map, not as a single value but as multiple values:\n",
    "\n",
    "![https://d2l.ai/_images/trans_conv.svg](https://d2l.ai/_images/trans_conv.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97e30bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (32, 28, 28, 1)\n",
      "resulting shape from using 32 filters, kernel size (7, 7) and strides of (1, 1):\n",
      "(32, 34, 34, 32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_filters=32\n",
    "kernel_size=(7,7)\n",
    "strides=(1,1)\n",
    "\n",
    "inputs = tf.ones((32,28,28,1))\n",
    "layer = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size=kernel_size,strides=strides)\n",
    "print(\"input shape:\", inputs.shape)\n",
    "print(f\"resulting shape from using {n_filters} filters, kernel size {kernel_size} and strides of {strides}:\\n{layer(inputs).shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
