{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6a2463",
   "metadata": {},
   "source": [
    "# Backpropagation from scratch with TensorFlow\n",
    "\n",
    "## (1A) Implementing your own network and its backward pass\n",
    "\n",
    "You are asked to write the backward pass for a different neural network architecture, similar to how I did it in the session 02 notebook. The difference is the number of units, the choice of activation function, and the loss function (here we use one that is used in binary classification).\n",
    "\n",
    "**Layer 1**: 2 inputs, 12 outputs\n",
    "\n",
    "**Nonlinearity 1**: max(0, x)\n",
    "\n",
    "**Layer 2**: 12 inputs, 32 outputs\n",
    "\n",
    "**Nonlinearity 2**: max(0, x)\n",
    "\n",
    "**Layer 3**: 32 inputs, 1 output\n",
    "\n",
    "**Nonlinearity3**: sigmoid(x)\n",
    "\n",
    "**Loss function**: $\\mathcal{L}_{\\text{Crossentropy}}(y, p)= - y \\circ \\text{log}(p) - (1-y) \\circ \\text{log}(1-p)$\n",
    "\n",
    "for a label $y \\in$ {0,1} and the model prediction $p$.\n",
    "\n",
    "The local derivative of $\\mathcal{L}_{\\text{Crossentropy}}$ w.r.t. $p$ is $-\\frac{y}{p} + \\frac{1-y}{1-p}$\n",
    "\n",
    "For the derivative of max(0,x) you can set the derivative value to 0 for x=0, even though it is mathematically not defined.\n",
    "You can use tf.nn.relu(x) for the non-linearity.\n",
    "\n",
    "## Step 1: Instantiate the weights\n",
    "\n",
    "You should instantiate tf.Variable objects for each weight and bias tensor. Then you should put them into a tuple of tuples, one inner tuple for each layer, comprising (weights, biases). You should call this tuple of tuples \"variables\".\n",
    "\n",
    "## Step 2: Write the forward pass, returning results from intermediate computations\n",
    "\n",
    "The function should take an input tensor, a target tensor and the nested variables tuple, perform the forward computation and return a tuple of all intermediate results, i.e. \n",
    "\n",
    "(loss, nonlinearity3_out, layer3_out, nonlinearity2_out, layer2_out, nonlinearity1_out, layer1_out, inputs)\n",
    "\n",
    "## Step 3: Write the compute_gradients function, chaining the gradients\n",
    "\n",
    "The function should take the data from the forward function, as well as the variables, the target and a batch size as arguments. It should compute the gradients like it was done in session 02 and then return the gradients in the same format as the variables (a tuple of tuples)\n",
    "\n",
    "## Step 4: Evaluate your implementation with the cell below\n",
    "If no error occurs, it means you succeeded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2dc9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use something like this for testing your results. x should have shape \n",
    "import tensorflow as tf\n",
    "\n",
    "# Instantiate variables for the layers\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# e.g.\n",
    "# weights_1 = tf.random.uniform((2, 12))\n",
    "# ...\n",
    "# variables = ((weights_1, bias_1), (weights_2, bias_2))\n",
    "\n",
    "def compute_forward_pass(inputs, targets, variables):\n",
    "    #YOUR CODE HERE\n",
    "    ...\n",
    "    \n",
    "def compute_gradients(variables, data_from_forward, target, batch_size=1):\n",
    "    # YOUR CODE HERE\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c8cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST YOUR SOLUTION:\n",
    "x = tf.constant([[0.3,0.9]])\n",
    "y = tf.constant([[1.0]])\n",
    "\n",
    "data_from_forward = compute_forward_pass(x, y, variables)\n",
    "your_solution_gradients = compute_gradients(variables, data_from_forward, y, batch_size=1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(variables)\n",
    "    data_from_forward = compute_forward_pass(x, y, variables)\n",
    "    loss = data_from_forward[0]\n",
    "gradients = tape.gradient(loss, variables)\n",
    "\n",
    "assert gradients[0][0]==your_solution_gradients[0][0], \"computed gradients do not match with the TensorFlow oracle!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
