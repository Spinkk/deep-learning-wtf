{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34717ab",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks in Tensorflow\n",
    "\n",
    "We use recurrent neural networks (RNNs) to process sequential data. That could be a video (stream of frames), a multi-variate time-series of sensor data, or sequences of symbols such as sentences in a natural language.\n",
    "\n",
    "Similarly to how we share weights across a spatial feature map in convolutions, RNNs also share weights, but across time and not necessarily across space (unless we construct a convolutional RNN).\n",
    "\n",
    "Let's assume we have a time-series of 10 features (e.g. **10 sensors**) and the sensors measured at **28 times**. The sequence data for this measurement has the shape **(28, 10)** without a batch dimension, and **(1, 28, 10)** with the batch dimension added (which is always needed).\n",
    "\n",
    "If our sequential data were videos (or a collection of frames), a batch would have the shape (batch_size, t, h, w, channels). The time dimension is generally in the second axis.\n",
    "\n",
    "Like convolutional networks, recurrence can be thought of as introducing a form of weight sharing (through time, not space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# create some dummy data (ANNs can fit to random noise very well!)\n",
    "\n",
    "# dataset size: 1024; sequence length: 28; features: 10\n",
    "x_train = np.random.normal(size=(1024, 28, 10))\n",
    "# target: 0 or 1, randomly assigned\n",
    "y_train = np.random.randint(low=0, high=2, size=(1024,1))\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "x_val = np.random.normal(size=(512, 28, 10))\n",
    "# target: 0 or 1, randomly assigned\n",
    "y_val = np.random.randint(low=0, high=2, size=(512,1))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(512).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c7bc0",
   "metadata": {},
   "source": [
    "A simple RNN could look something like this: $$f(f(f(f(s_0)+s_1)+s_2)+s_3) ...,$$ where f could be a Dense layer, and $s_t$ is the feature vector in our sequence at time-point t.\n",
    "\n",
    "This looks just like a feedforward model, except we add new features from the data sequence, and we re-use the same layer over and over again, for all time-steps of processing.\n",
    "\n",
    "## Unrolled RNNs\n",
    "\n",
    "One way to implement an RNN based on this understanding is to construct an \"unrolled\" network for a fixed sequence length, just like we would code a feed-forward model. The only difference is that slices of the model input are added at different parts in the forward computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnrolledRNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer=tf.keras.layers.Dense(10, activation=\"tanh\")\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        ...\n",
    "    def call(self, sequence, training=False):\n",
    "        # t=0\n",
    "        layer_state_t0 = self.layer(sequence[:,0,:] + tf.zeros_like(sequence[:,0,:]))\n",
    "        layer_state_t1 = self.layer(layer_state_t0 + sequence[:,1,:])\n",
    "        layer_state_t2 = self.layer(layer_state_t1 + sequence[:,2,:])\n",
    "        layer_state_t3 = self.layer(layer_state_t2 + sequence[:,3,:])\n",
    "        \n",
    "        return self.output_layer(layer_state_t3)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96100d",
   "metadata": {},
   "source": [
    "This approach at building an RNN is very useful and allows for fine-grained control over what is happening when in the computational graph. One downside is that we need to determine how long the sequences are. It also assumes that we get the full sequence as input at once and not incrementally. In cases where we want an RNN to control an agent (e.g. a car) in an environment, we need to have a more dynamic RNN that can maintain its state across multiple calls. \n",
    "\n",
    "For this we can write a for loop and write the state of the layer to a model attribute after each time-step. Since tensorflow is rather picky about stateful operations, it is highly recommended to not write the for loop yourself, and instead use in-built tensorflow/keras abstractions:\n",
    "\n",
    "We define an RNN \"cell\" layer that defines the computations that we want our RNN to apply at every time-step as we loop over the sequence, updating the RNN state.\n",
    "\n",
    "This **RNN-cell layer** needs to have a **state_size** property, an **output_size** property, and a **get_initial_state** method.\n",
    "\n",
    "Below I define a somewhat more advanced RNN Cell layer that has two layers , using linear projections (dense layers), orthogonally initialized weight matrices, \"layer normalization\" layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2205d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(tf.keras.layers.AbstractRNNCell):\n",
    "\n",
    "    def __init__(self, recurrent_units_1, recurrent_units_2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.recurrent_units_1 = recurrent_units_1\n",
    "        self.recurrent_units_2 = recurrent_units_2\n",
    "        \n",
    "        self.linear_1 = tf.keras.layers.Dense(recurrent_units_1)\n",
    "        self.linear_2 = tf.keras.layers.Dense(recurrent_units_2)\n",
    "        \n",
    "        # first recurrent layer in the RNN\n",
    "        self.recurrent_layer_1 = tf.keras.layers.Dense(recurrent_units_1, \n",
    "                                                       kernel_initializer= tf.keras.initializers.Orthogonal(\n",
    "                                                           gain=1.0, seed=None),\n",
    "                                                       activation=tf.nn.tanh)\n",
    "        # layer normalization for trainability\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        # second recurrent layer in the RNN\n",
    "        self.recurrent_layer_2 = tf.keras.layers.Dense(recurrent_units_2, \n",
    "                                                       kernel_initializer= tf.keras.initializers.Orthogonal(\n",
    "                                                           gain=1.0, seed=None), \n",
    "                                                       activation=tf.nn.tanh)\n",
    "        # layer normalization for trainability\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return [tf.TensorShape([self.recurrent_units_1]), \n",
    "                tf.TensorShape([self.recurrent_units_2])]\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return [tf.TensorShape([self.recurrent_units_2])]\n",
    "    \n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        return [tf.zeros([self.recurrent_units_1]), \n",
    "                tf.zeros([self.recurrent_units_2])]\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        # unpack the states\n",
    "        state_layer_1 = states[0]\n",
    "        state_layer_2 = states[1]\n",
    "        \n",
    "        # linearly project input\n",
    "        x = self.linear_1(inputs) + state_layer_1\n",
    "        \n",
    "        # apply first recurrent kernel\n",
    "        new_state_layer_1 = self.recurrent_layer_1(x)\n",
    "        \n",
    "        # apply layer norm\n",
    "        x = self.layer_norm_1(new_state_layer_1)\n",
    "        \n",
    "        # linearly project output of layer norm\n",
    "        x = self.linear_2(x) + state_layer_2\n",
    "        \n",
    "        # apply second recurrent layer\n",
    "        new_state_layer_2 = self.recurrent_layer_2(x)\n",
    "        \n",
    "        # apply second layer's layer norm\n",
    "        x = self.layer_norm_2(new_state_layer_2)\n",
    "        \n",
    "        # return output and the list of new states of the layers\n",
    "        return x, [new_state_layer_1, new_state_layer_2]\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"recurrent_units_1\": self.recurrent_units_1, \n",
    "                \"recurrent_units_2\": self.recurrent_units_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bf3d3",
   "metadata": {},
   "source": [
    "### Using the RNN layer in a model\n",
    "\n",
    "In this example, I use the model.compile and model.fit methods. For this reason, unlike before, we do not specify the optimizer and loss function in the model's constructor method. \n",
    "\n",
    "The compile method adds an optimizer to the model, as well as a loss function. As a consequence, the train_step and test_step methods look (only marginally) different from what you've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8422b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_cell = RNNCell(recurrent_units_1=24,\n",
    "                                recurrent_units_2=48)\n",
    "        \n",
    "        # return_sequences collects and returns the output of the rnn_cell for all time-steps\n",
    "        # unroll unrolls the network for speed (at the cost of memory)\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnn_cell, return_sequences=False, unroll=True)\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                             tf.keras.metrics.BinaryAccuracy()]\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_state()\n",
    "        \n",
    "    def call(self, sequence, training=False):\n",
    "        \n",
    "        rnn_output = self.rnn_layer(sequence)\n",
    "        \n",
    "        return self.output_layer(rnn_output)\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        \n",
    "        \"\"\"\n",
    "        Standard train_step method, assuming we use model.compile(optimizer, loss, ...)\n",
    "        \"\"\"\n",
    "        \n",
    "        sequence, label = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self(sequence, training=True)\n",
    "            loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        self.metrics[0].update_state(loss)\n",
    "        self.metrics[1].update_state(label, output)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        \n",
    "        \"\"\"\n",
    "        Standard test_step method, assuming we use model.compile(optimizer, loss, ...)\n",
    "        \"\"\"\n",
    "        \n",
    "        sequence, label = data\n",
    "        output = self(sequence, training=False)\n",
    "        loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n",
    "                \n",
    "        self.metrics[0].update_state(loss)\n",
    "        self.metrics[1].update_state(label, output)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4667bed",
   "metadata": {},
   "source": [
    "### Instantiating the model and calling the compile method\n",
    "In the compile method, we need to specify the optimizer and the loss object. Importantly, we need to pass an instance of the optimizer and the loss, not the class (i.e. tf.keras.losses.BinaryCrossentropy() and not tf.keras.losses.BinaryCrossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b37a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# compile the model (here, adding a loss function and an optimizer)\n",
    "model.compile(optimizer = optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097886e",
   "metadata": {},
   "source": [
    "# Using the fit method to train the RNN model on random noise sequences\n",
    "\n",
    "So far we used custom training loop functions to train our model. With the model.fit abstraction, we can skip this step. Any custom non-standard behavior that we want to integrate can be implemented with callbacks (tf.keras.callbacks). These can be called after every batch, before every batch, before every epoch, or after every epoch, and you can write your own callbacks that e.g. reduce certain hyperparameters over time as training progresses.\n",
    "\n",
    "Here we want to log all keras metrics of our model to the tensorboard, just like we did before. For this, we use the TensorBoard callback, for which we need to specify the log directory filepath.\n",
    "\n",
    "We might also want to save our model weights every n epochs, so we can continue training (e.g. if gradients explode and weights become nan values) or we want to analyze what our model learned during a particular phase of training later. There is a callback for checkpointing: tf.keras.callbacks.ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"RNN_noise\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logging_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"./logs/{EXPERIMENT_NAME}/{current_time}\")\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    initial_epoch=25,\n",
    "                    epochs=50,\n",
    "                    callbacks=[logging_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b044cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the complete model (incl. optimizer state, loss function, metrics etc.)\n",
    "model.save(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and resume training where we had to stop\n",
    "loaded_model = tf.keras.models.load_model(\"saved_model\", custom_objects={\"RNNCell\": RNNCell,\n",
    "                                                                         \"RNNModel\": RNNModel})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4565288",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "\n",
    "Since we used the logging callback, we can also visualize our data on the tensorboard. The model.fit method however provides us with a history object, containing a dictionary with the metrics across all epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend(labels=[\"training\",\"validation\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Binary Crossentropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4249cd",
   "metadata": {},
   "source": [
    "# Look at the tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6496ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=\"logs/RNN_noise\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
