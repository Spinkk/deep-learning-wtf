{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f14e884",
   "metadata": {},
   "source": [
    "# Modules, Layers, and Models\n",
    "\n",
    "In this session we will look at how the following differ in their functionality:\n",
    "- tf.Module\n",
    "- tf.keras.Layer\n",
    "- tf.keras.Model\n",
    "\n",
    "\n",
    "## tf.Module\n",
    "\n",
    "The basic building block in Tensorflow to construct a neural network model is the [**Module**](https://www.tensorflow.org/api_docs/python/tf/Module). This is a low-level tensorflow class that itself does not provide all the functionality that comes with the high level abstraction library that is integrated into tensorflow: [Keras](keras.io) but is now also available as an API for other backends such as PyTorch and JAX. These higher level tools nevertheless build on the Module and only further extend its functionality.\n",
    "\n",
    "What does a module do?\n",
    "\n",
    "A neural network is both a **collection of computational operations on some inputs** and a **set of trainable or non-trainable parameters**. A module allows to organize this structure in a way that makes it **easy to access all variables** in a convenient way. To do so, we will use subclassing, creating a new class based on the base tf.Module class and inheriting its properties. \n",
    "\n",
    "It works like this:\n",
    "\n",
    "```python\n",
    "class MyNetwork(tf.Module):\n",
    "    def __init__(self, architecture_arguments, name=None):\n",
    "        super().__init__(name=name)\n",
    "```\n",
    "(Note how the name of the class and the first argument in super(..., self) have to match)\n",
    "\n",
    "Now we can start to add variables to the module in the constructor (init). To do so we can use **tf.Variable** like this, while specifying whether it should be flagged as trainable or not:\n",
    "\n",
    "```python\n",
    "class MyNetwork(tf.Module):\n",
    "    def __init__(self, architecture_arguments=None, name=None):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.my_variable = tf.Variable([1.,1.5,1.2,3.], trainable=True)\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "Here the variable is a rank one tensor of shape (4). Now that we have a non-empty model class, we can see what makes this module structure useful. We can collect and access all stored variables at once by calling \"model.variables\" on the model.\n",
    "\n",
    "This makes updating variables (usually weights and biases) convenient, because variables can be contained in nested structures and sub-modules. We can also obtain only those variables flagged as trainable with \"model.trainable_variables\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881bd95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([1. , 1.5, 1.2, 3. ], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([3., 2.], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.21], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([9., 2.], dtype=float32)>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# set a global seed for the random number generator to make results reproducible\n",
    "tf.random.set_seed(10)\n",
    "\n",
    "class MyNetwork(tf.Module):\n",
    "    def __init__(self, architecture_arguments=None, name=None):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.my_variable = tf.Variable([1.,1.5,1.2,3.], trainable=True)\n",
    "        \n",
    "        # any nested data structure containing tf.Variables works!\n",
    "        self.variable_dict = {\"A\": ([tf.Variable([3.,2.], trainable=False)]), \n",
    "                              \"B\": [[[[[tf.Variable([1.21])]]]]], \n",
    "                              \"C\": {\"B1\": tf.Variable([9.,2.])}}\n",
    "        \n",
    "        \n",
    "        \n",
    "model = MyNetwork()\n",
    "\n",
    "model.variables\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f94a3",
   "metadata": {},
   "source": [
    "By itself this model does not yet do anything with the variables. All the computational structure arises in a separate call method. There we specify how the inputs and the variables are used in a sequence of computations such that an output is produced. A simple call method could look like this:\n",
    "\n",
    "```python\n",
    "class MyNetwork(tf.Module):\n",
    "    def __init__(self, architecture_arguments=None, name=None):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.my_variable = tf.Variable([1.,1.5,1.2,3.], trainable=True)\n",
    "        \n",
    "    @tf.function    \n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        x = inputs @ tf.transpose(self.my_variable)\n",
    "        \n",
    "        return tf.nn.relu(x)\n",
    "        \n",
    "```\n",
    "\n",
    "Here we've set a standard \"training\" argument in the call method to default to False. This is because later on we will want the computations to differ between training and inference time.\n",
    "\n",
    "We have also used [**tf.function**](https://www.tensorflow.org/api_docs/python/tf/function) as a decorator. This makes the computations run on a graph which is more efficient.\n",
    "\n",
    "Let's test the model on randomly generated input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(tf.Module):\n",
    "    def __init__(self, architecture_arguments=None, name=None):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.my_variable = tf.Variable([[1., 1.5, 1.2, 3.]], trainable=True)\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, inputs, training=False):\n",
    "        \n",
    "        x = inputs @ tf.transpose(self.my_variable)\n",
    "\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "model = MyNetwork()\n",
    "\n",
    "# a \"batch size\" of 1 example with 4 features\n",
    "input_data = tf.random.uniform((1,4)) \n",
    "\n",
    "out = model(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335bdad",
   "metadata": {},
   "source": [
    "Any Module can encapsulate other modules and we can still directly access all variables, trainable or non_trainable in the same way. To see this, let's construct the same module except this time, the output shape should be the input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(tf.Module):\n",
    "    def __init__(self, input_dim, output_dim, name=None):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.my_variable = tf.Variable(tf.random.normal(shape=(input_dim, output_dim)), trainable=True, name=name)\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, inputs, training=False):\n",
    "        \n",
    "        x = inputs @ self.my_variable\n",
    "\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "# keep input dimensionality for output\n",
    "model = MyModule(input_dim=4, output_dim=4)\n",
    "\n",
    "input_data = tf.random.uniform((1,4))\n",
    "\n",
    "# apply the model multiple times\n",
    "model(model(model(model(model(input_data)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354aa57",
   "metadata": {},
   "source": [
    "We can now build a multi layer perceptron (without a bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(tf.Module):\n",
    "    def __init__(self, input_dim, output_dim, name=None):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.submodule_1 = MyModule(input_dim = input_dim, output_dim = 10, name=\"layer1\")\n",
    "        self.submodule_2 = MyModule(input_dim = 10, output_dim = 20, name=\"layer2\")\n",
    "        self.submodule_3 = MyModule(input_dim = 20, output_dim = output_dim, name=\"layer3\")\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, inputs, training=False):\n",
    "        \n",
    "        x = self.submodule_1(inputs, training)\n",
    "        x = self.submodule_2(x, training)\n",
    "        output = self.submodule_3(x, training)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a81d3f",
   "metadata": {},
   "source": [
    "Here we have passed down the training argument to the submodules in the call method. It does not yet do anything but we will need it for later. We also created the class in a way that allows to build different MLPs for different input and output sizes. Additionally, since it can become very difficult to know which variable belongs to which submodule, a convention is to give them **names**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e980eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"input_dim\" : 4,\n",
    "          \"output_dim\": 1}\n",
    "\n",
    "model = MyNetwork(**kwargs)\n",
    "\n",
    "input_data = tf.random.uniform((1,4)) \n",
    "\n",
    "print(model(input_data))\n",
    "\n",
    "print(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c8fa9",
   "metadata": {},
   "source": [
    "One more feature that comes with the tf.Module class is to use a naming context manager that will automatically tag all variables with the same module name, making it easier for modules with multiple variable tensors. It can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a932fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(tf.Module):\n",
    "    def __init__(self, input_dim, output_dim, name=None):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        if not name:\n",
    "            name=\"noname_module\"\n",
    "            \n",
    "        with tf.name_scope(name) as scope:\n",
    "\n",
    "            self.weights = tf.Variable(tf.random.normal(shape=(input_dim, output_dim)), trainable=True, \n",
    "                                           name=\"weights\")\n",
    "            self.bias = tf.Variable(tf.zeros(shape=(1,output_dim)), trainable=True,\n",
    "                                    name=\"bias\")\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, inputs, training=False):\n",
    "        \n",
    "        x = inputs @ self.my_variable + self.bias\n",
    "\n",
    "        return tf.nn.relu(x)\n",
    "    \n",
    "dense = MyModule(input_dim=4, output_dim=1, name=\"dense_layer\")\n",
    "\n",
    "dense.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c9273",
   "metadata": {},
   "source": [
    "By subclassing the tf.Module class and building your own submodules, you can build any neural network architecture that you like. There is no _need_ to use predefined Keras layers or the Model class since these only provide extra functionality on top of what tf.Module provides.\n",
    "\n",
    "# The Layer Class\n",
    "\n",
    "The tf.keras.layers.Layer base class inherits from tf.Module but extends its functionality in several ways.\n",
    "\n",
    "Perhaps the most important extension is a separate [**build method**](https://www.tensorflow.org/guide/intro_to_modules#keras_models_and_layers) which allows to keep a module and its variables agnostic to the input shape until it is first called on a specific input. This means that the constructor no longer instantiates the variables but only the general structure of the module (e.g. which submodules are part of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_units, activation_function, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # no variables created\n",
    "        self.n_units = n_units\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = tf.Variable(tf.random.normal([input_shape[-1], self.n_units]), name='weights')\n",
    "        self.b = tf.Variable(tf.zeros([self.n_units]), name='bias')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs @ self.w + self.b\n",
    "        return self.activation_function(x)\n",
    "\n",
    "# instantiate the layer\n",
    "dense_layer = Dense(n_units=10, activation_function=tf.nn.relu)\n",
    "\n",
    "# it has no variables\n",
    "print(f\"dense_layer variables:{dense_layer.variables}\")\n",
    "\n",
    "# call it on an input to create weights suitable for this input\n",
    "dense_layer(tf.random.uniform(shape=(32,16)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5554a",
   "metadata": {},
   "source": [
    "Besides the build method, keras layers come with a number of extra functionality that we will explore later in the course:\n",
    "\n",
    "- adding and collecting regularization losses\n",
    "- metrics and bookkeeping\n",
    "- handling masking and training arguments\n",
    "- access submodules in a structured way\n",
    "\n",
    "# The Model Class\n",
    "\n",
    "The Keras Model class is a very useful tool that inherits from the Layer class but adds even more functionality to it for much more convenient use, which is great for standard applications.\n",
    "\n",
    "It allows to:\n",
    "\n",
    "- Use a fit method to do the training\n",
    "- compile the model with metrics, a loss and an optimizer\n",
    "- Save the model's weights\n",
    "- Pre-defined methods for inference and for evaluation\n",
    "- Models can also be submodules of further modules.\n",
    "- Access all layers in the model.\n",
    "- Contain optimizer and training logic within the model\n",
    "- Contain training and validation metrics within the model\n",
    "- Additionally we can create models without subclassing with the [Functional API](https://www.tensorflow.org/guide/keras/functional?hl=en)\n",
    "\n",
    "In short: You can have everything you need for training in one object (except for the data itself) and make many routine things faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27252bef",
   "metadata": {},
   "source": [
    "# Why use the low level features of tensorflow if you have the convenient tools that Keras comes with?\n",
    "\n",
    "It seems very convenient to just use the high level Model class and reduce deep learning to a few lines of code and a call to model.fit(). While this is certainly true, it is very important to know what is going on at a lower level such that you can implement non-standard applications. \n",
    "\n",
    "Keras is developed in a way that allows many of these things by tightly integrating the high level features with low level control. For instance you can write a custom tf.Module and use it as a submodule in a tf.keras.layers.Layer or tf.keras.Model. You could even use a model that you have written at a very low level and encapsulate it in the Model class to obtain convenient training and evaluation methods.\n",
    "\n",
    "Since you will not need most of the functionality provided by the layer and model classes, you will learn the most by restricting yourself to the tf.Module class. You can still include Keras layers such as the Dense layer or the Conv2D layer within your tf.Module model.\n",
    "\n",
    "Focusing on the tf.Module class (or at least restricting your use of the keras model class to its functionalities) at first also allows you to understand code in other frameworks such as [PyTorch](https://pytorch.org/) which do not have the layer and model abstractions. For instance, in PyTorch you are required to always specify input and output dimensions on your layers explicitly. When using Keras Layers, this is automatically inferred in the build method. This can lead to more confusion when required to fix non-matching shapes in a model because you become reliant on Keras determining the required dimensions of variables.\n",
    "\n",
    "For this reason, having some experience without the nice tools that Keras provides will equip you with the right perspective for when you will be working with much more advanced models in the future and need to debug them.\n",
    "\n",
    "\n",
    "**TL;DR: Keras is great and we love it but you need to work with the low level features to learn how to debug complex models (important for your projects).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
