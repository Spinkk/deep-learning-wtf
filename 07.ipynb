{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3d5349",
   "metadata": {},
   "source": [
    "# Putting it together:  Using TensorBoard to log training data of a subclassed model with keras metrics with a custom training loop\n",
    "\n",
    "We will define a subclassed feed forward fully connected model and store loss and accuracy for both training and validation data to the TensorBoard. \n",
    "\n",
    "To do this in a clean way, we implement the keras metrics that keep track of loss and accuracy in each epoch for us as part of the model. We also define the train and test steps as methods inside the model rather than as external functions. Doing so will move us one step closer to being able to use the in-built training and evaluation methods that come with Tensorflow/Keras, that is the compile and fit methods.\n",
    "\n",
    "To use train_step and test_step as methods of the model, we need to have the loss-function, the metrics, and the optimizer as parts of the model, which is why we define them in the init method.\n",
    "\n",
    "Note that we need to update the metrics after each training example and reset the metrics after each epoch or before evaluating our model on the validation data set.\n",
    "\n",
    "Also note that the metrics_list contains a mean metric for the loss, which does not take targets and predictions as arguments in its update_state method, but just a scalar. For this reason, we treat it differently from the remaining metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafec094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# in a notebook, load the tensorboard extension, not needed for scripts\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.metrics_list = [\n",
    "                        tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "                        tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\") \n",
    "                       ]\n",
    "        \n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)   \n",
    "        \n",
    "        # define layers\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.layer1 = tf.keras.layers.Dense(32,activation=\"relu\")\n",
    "        self.layer2 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.layer3 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.layer4 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.output_layer = tf.keras.layers.Dense(10, activation=None)\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        # flatten images to vectors\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        out = self.output_layer(x)\n",
    "       \n",
    "        return out\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    #@tf.function\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        x, targets = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(x, training=True)\n",
    "            \n",
    "            loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(targets,predictions)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    #@tf.function\n",
    "    def test_step(self, data):\n",
    "\n",
    "        x, targets = data\n",
    "        predictions = self(x, training=False)\n",
    "        loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
    "\n",
    "        self.metrics[0].update_state(loss)\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(targets, predictions)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac1ba1",
   "metadata": {},
   "source": [
    "# Preparing the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c716440",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(\"fashion_mnist\", as_supervised=True)\n",
    "\n",
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"test\"]\n",
    "\n",
    "train_ds = train_ds.map(lambda x,y: (x/255, tf.one_hot(y, 10, dtype=tf.float32)),\\\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(5000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(lambda x,y: (x/255, tf.one_hot(y, 10, dtype=tf.float32)),\\\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(5000).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = FFN()\n",
    "\n",
    "# run model on input once so the layers are built\n",
    "model(tf.keras.Input((28,28,1)));\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5b494",
   "metadata": {},
   "source": [
    "# Instantiate the file-writers for the training\n",
    "\n",
    "We store the tensorboard logs to a folder with a meaningful name (e.g. name of training run + date and time). Additionally, when running experiments, you want to save a config file that can be associated with these logs, containing all information about the architecture and hyperparameters that were used. To be extra sure, you could also make a copy of the code that was used. Not knowing which settings lead to which results should be avoided by all means. A good tool for configurations is the [Hydra library](https://hydra.cc/) which also allows to have objects and their arguments as part of a config file (e.g. activation functions, optimizers etc.).\n",
    "\n",
    "- We create a train writer and a validation writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81860eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where to save the log\n",
    "config_name= \"Run-42\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187c56c",
   "metadata": {},
   "source": [
    "# Writing the training loop\n",
    "\n",
    "Note that you need to re-run the above cell (and hence update the time-stamp) if you don't want to over-write the data of the previous training-run.\n",
    "\n",
    "If you use keras metrics, do not forget to reset the states between train and validation and between epochs.\n",
    "We use metric.update_states(...) to update a metric. This usually means we update the running average with the new value. There also exist keras metrics that can also compute scores such as CategoricalAccuracy, TopKCategoricalAccuracy.\n",
    "\n",
    "We use TQDM to see the progress of each epoch and the estimate of how much time it will take.\n",
    "\n",
    "Instead of looking at the printed losses and accuracies, we can look at the TensorBoard plots which will be updated after every epoch. This requires us to open and load the tensorboard *before* starting the training or to open the tensorboard from a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import tqdm\n",
    "\n",
    "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        \n",
    "        # Training:\n",
    "        \n",
    "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
    "            metrics = model.train_step(data)\n",
    "            \n",
    "            # logging the validation metrics to the log file which is used by tensorboard\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics (requires a reset_metrics method in the model)\n",
    "        model.reset_metrics()    \n",
    "        \n",
    "        # Validation:\n",
    "        for data in val_ds:\n",
    "            metrics = model.test_step(data)\n",
    "        \n",
    "            # logging the validation metrics to the log file which is used by tensorboard\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "                    \n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics\n",
    "        model.reset_metrics()\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddc898",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training loop \n",
    "training_loop(model=model, \n",
    "                train_ds=train_ds, \n",
    "                val_ds=val_ds, \n",
    "                epochs=15, \n",
    "                train_summary_writer=train_summary_writer, \n",
    "                val_summary_writer=val_summary_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd9525",
   "metadata": {},
   "source": [
    "# Saving and loading a subclassed model\n",
    "\n",
    "Because training deep neural networks can take multiple days, weeks or even months, we want to save checkpoints in between. This is especially useful if you use Google Colab and you save the model directly to your Google Drive folder. That way you don't lose any progress if your runtime gets closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337745ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model with a meaningful name\n",
    "model.save_weights(f\"saved_model_{config_name}\", save_format=\"tf\")\n",
    "\n",
    "# load the model:\n",
    "# instantiate a new model from our CNN class\n",
    "loaded_model = FFN()\n",
    "\n",
    "# build the model\n",
    "inp= tf.keras.Input((28,28,1))\n",
    "loaded_model(inp)\n",
    "\n",
    "# load the model weights to continue training. \n",
    "loaded_model.load_weights(f\"saved_model_{config_name}\");\n",
    "\n",
    "# continue training (but: optimizer state is lost)\n",
    "\n",
    "# run the training loop \n",
    "training_loop(model=loaded_model, \n",
    "                train_ds=train_ds, \n",
    "                val_ds=val_ds, \n",
    "                epochs=10, \n",
    "                train_summary_writer=train_summary_writer, \n",
    "                val_summary_writer=val_summary_writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
