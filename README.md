# Learning Deep Learning with TensorFlow

This is a compiled and revised repository of my teaching material for a course on deep learning with TensorFlow in 2021/22 and 2022/23 and is meant as a self-contained resource for self-study. 

Sessions 14 and 15 as well as exercises for the sessions are not yet available in this repository. They will be added when I find time to add content that is suitable for self-study.

Course content and recommended time to do exercises:

|     Session     |    Content                             |              Exercise                 |
|-----------------|----------------------------------------|---------------------------------------|
|   00            |    Basic tensor operations in TensorFlow + Prerequisites|               Exercise0               |
|   01            |    From biological neurons to logic gates, to activation functions to universal function approximation (build your first ANN from scratch)                            |                 Exercise01                      |
|   02            |   Learning in ANNs: Gradient Descent, Backpropagation, and Automatic Differentiation (build your first ANN from scratch, including backpropagation and training loop)                                |                Exercise02                       |  
|   03            |       Basic usage of TensorFlow's automatic differentiation: The GradientTape context manager                      |            -                           |
|   04            |       Modules, Layers, and Models. An introduction to the Keras Subclassing API                |  -  |
|    05            |    Keras metrics for keeping track of losses, accuracies etc.              |     -          |
|    06            |      Loss functions and optimizers           |       Exercise03        |
|    07            |      Putting it together: Using TensorBoard to log training data and implementing a subclassed model using keras metrics and a custom training loop.           |       -        |
|    08            |     Convolutional Neural Networks (incl. interactive widget)           |       Exercise04        |
|    09            |     Regularization: Avoiding overfitting with L1/L2 penalties, dropout, normalization and data augmentation           |       Exercise05        |
|    10            |     Optimization difficulties: Vanishing and exploding gradients. Weight initialization, normalization and residual/skip connections as partial solutions           |       Exercise06        |
|    11            |     Recurrent Neural Networks: From unrolled recurrence to dynamically unrolled custom recurrent cells           |       Exercise07        |
|    12            |     Autoencoder           |       Exercise08        |
|    13            |     Generative Models           |       Exercise09        |
|    (14)            |     Transformers and NLP           |       Exercise10        |
|    (15)            |     Deep Reinforcement Learning           |       Exercise11        |
