{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33028f9c",
   "metadata": {},
   "source": [
    "# Regularization: Avoiding overfitting\n",
    "\n",
    "This session investigates different methods of regularization. There are many more methods than what I show here, but these are common/classical methods that come to mind when trying to make a model generalize better without acquiring additional training data or undertaking pre-training (these are preferable options if possible). The general aim is to obtain models that generalize better to data not used in training. It is a topic of central importance because neural networks tend to overfit very much and typically need large amounts of data to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import keras_cv # install keras_cv with pip for data augmentation (soon to be integrated into keras core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4245583d",
   "metadata": {},
   "source": [
    "# Load cifar10 data\n",
    "\n",
    "We load the cifar 10 data set and map some functions to its elements such that the labels are float32 one-hot vectors and the images are normalized to have float32 values between 0 and 1. We include the option for data augmentation, which is only applied to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771789c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10(batch_size, augmentation_model=None):\n",
    "    \"\"\"\n",
    "    Load and prepare CIFAR-10 as a tensorflow dataset.\n",
    "    Returns a train and a validation dataset.\n",
    "    Args:\n",
    "    batch_size (int)\n",
    "    \"\"\"\n",
    "    train_ds, val_ds = tfds.load('cifar10', split=['train', 'test'], shuffle_files=True)\n",
    "\n",
    "    one_hot = lambda x: tf.one_hot(x, 10)\n",
    "\n",
    "    map_func = lambda x,y: (tf.cast(x, dtype=tf.float32)/255.,\n",
    "                            tf.cast(one_hot(y),tf.float32))\n",
    "\n",
    "    map_func_2 = lambda x: (x[\"image\"],x[\"label\"])\n",
    "\n",
    "    train_ds = train_ds.map(map_func_2).map(map_func).cache()\n",
    "    val_ds   = val_ds.map(map_func_2).map(map_func).cache()\n",
    "    \n",
    "    train_ds = train_ds.shuffle(4096).batch(batch_size)\n",
    "    val_ds   = val_ds.shuffle(4096).batch(batch_size)\n",
    "    if augmentation_model:\n",
    "        train_ds = train_ds.map(lambda x,y : (augmentation_model(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return (train_ds.prefetch(tf.data.AUTOTUNE), val_ds.prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828eb25c",
   "metadata": {},
   "source": [
    "# Define Keras model with options for regularization\n",
    "\n",
    "- Dropout layers randomly drop certain units in their input but keep the statistics constant\n",
    "\n",
    "\n",
    "- Batch normalization or other normalization rescale the input (in the case of batch norm, according to statistics over the batch dimension)\n",
    "\n",
    "\n",
    "- L1 and L2 regularization add a penalty to the loss function for high parameter values (usually a sign of overfitting, which is why we track the Frobenius norm of the weights, which is the Euclidean norm for matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9699ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(tf.keras.Model):\n",
    "    def __init__(self, L2_reg=0, dropout_rate=0, batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        kernel_regularizer=tf.keras.regularizers.L2(L2_reg) if L2_reg else None\n",
    "        self.dropout_rate = dropout_rate\n",
    "        if self.dropout_rate:\n",
    "            self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "            \n",
    "        self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), \n",
    "            tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "        if batch_norm:\n",
    "            \n",
    "                self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), \n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Flatten(),\n",
    "                                   #tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "        # metrics to update\n",
    "        self.frobenius_metric = tf.keras.metrics.Mean(name=\"total_frobenius_norm\")\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layer_list[:-1]:\n",
    "            x = layer(x, training=training)\n",
    "            if self.dropout_rate:\n",
    "                x = self.dropout_layer(x, training)\n",
    "        \n",
    "        return self.layer_list[-1](x)\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    def compute_frobenius(self):\n",
    "        frobenius_norm = tf.zeros((1,))\n",
    "        for var in self.trainable_variables:\n",
    "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
    "        return frobenius_norm\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self(x, training=True)\n",
    "            loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        self.frobenius_metric.update_state(self.compute_frobenius())\n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.accuracy_metric.update_state(target, prediction)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        x, target = data\n",
    "        prediction = self(x, training=False)\n",
    "        loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "        \n",
    "        self.frobenius_metric.update_state(self.compute_frobenius())\n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.accuracy_metric.update_state(target, prediction)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc7de7",
   "metadata": {},
   "source": [
    "## Training the model without any extra regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579411ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = get_cifar10(32)\n",
    "model = ConvModel()\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "history_of = model.fit(train_ds, validation_data=val_ds, epochs=25)\n",
    "\n",
    "plt.plot(history_of.history[\"total_frobenius_norm\"]/np.max(history_of.history[\"total_frobenius_norm\"]) * np.max(history_of.history[\"val_loss\"]))\n",
    "plt.plot(history_of.history[\"val_loss\"])\n",
    "plt.plot(history_of.history[\"loss\"])\n",
    "plt.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "plt.savefig(\"convnet_no_reg.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aa04ae",
   "metadata": {},
   "source": [
    "We see that the model overfits the data, that is, the performance on the training set gets increasingly better than on the validation set. Together with this, the total norm of the weights keeps increasing - a sign of memorization. Regularization methods can either directly address this by adding a penalty to the loss function for high parameter coefficients, or indirectly by adding noise somewhere in learning (e.g. dropout, batch norm, data augmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d787a",
   "metadata": {},
   "source": [
    "## Training the same model with L2 regularization\n",
    "\n",
    "L2 regularization adds a fraction of the euclidean norm of the weight matrices to the loss function, such that lower weight values lead to a lower loss, disencouraging memorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36564be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = get_cifar10(32)\n",
    "model = ConvModel(L2_reg=0.001)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "history_reg = model.fit(train_ds, validation_data=val_ds, epochs=25)\n",
    "\n",
    "plt.plot(history_reg.history[\"total_frobenius_norm\"]/np.max(history_reg.history[\"total_frobenius_norm\"]) * np.max(history_reg.history[\"val_loss\"]))\n",
    "plt.plot(history_reg.history[\"val_loss\"])\n",
    "plt.plot(history_reg.history[\"loss\"])\n",
    "plt.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "plt.savefig(\"convnet_l2_reg.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50271c3",
   "metadata": {},
   "source": [
    "## Training the same model with only data augmentation\n",
    "\n",
    "There are many different kinds of data augmentation: random cropping, randomly shifting the hue, rotating, zooming, flipping axes, cutting up images and mixing them across a batch, adding noise, introducing jpeg artifacts of different sorts, changing the contrast and brightness, etc. etc.\n",
    "\n",
    "Just as having a larger data set leads to better generalization, data augmentation does the same, albeit to a lesser degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c895f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_model = tf.keras.Sequential([keras_cv.layers.RandAugment(value_range=[0,1],magnitude=0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = get_cifar10(32, augmentation=augmentation_model)\n",
    "model = ConvModel()\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "history_augment = model.fit(train_ds, validation_data=val_ds, epochs=25)\n",
    "\n",
    "plt.plot(history_augment.history[\"total_frobenius_norm\"]/np.max(history_augment.history[\"total_frobenius_norm\"]) * np.max(history_augment.history[\"val_loss\"]))\n",
    "plt.plot(history_augment.history[\"val_loss\"])\n",
    "plt.plot(history_augment.history[\"loss\"])\n",
    "plt.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "plt.savefig(\"convnet_augment_reg.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc69b71",
   "metadata": {},
   "source": [
    "## Training the same model with only dropout between layers\n",
    "\n",
    "Dropout drops activation values in the preceding layer, preventing over-reliance on specific individual units, which is sometimes thought to be a hallmark of memorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe434f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = get_cifar10(32, augmentation=None)\n",
    "model = ConvModel(L2_reg=0, dropout_rate=0.5)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "history_dropout = model.fit(train_ds, validation_data=val_ds, epochs=25)\n",
    "\n",
    "plt.plot(history_dropout.history[\"total_frobenius_norm\"]/np.max(history_dropout.history[\"total_frobenius_norm\"]) * np.max(history_dropout.history[\"val_loss\"]))\n",
    "plt.plot(history_dropout.history[\"val_loss\"])\n",
    "plt.plot(history_dropout.history[\"loss\"])\n",
    "plt.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "plt.savefig(\"convnet_dropout_reg.svg\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
