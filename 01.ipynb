{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd22d0e2",
   "metadata": {},
   "source": [
    "# Course outline\n",
    "\n",
    "This is compiled material from the course Implementing ANNs with TensorFlow which I have co-taught in 2021/22 and 2022/23 at OsnabrÃ¼ck University. Some material was presented in a different format and is thus not yet part of this repository. That includes weekly homeworks as well as topics such as Autoencoders, NLP, ResNet and DenseNet architectures and Deep Reinforcement Learning.\n",
    "\n",
    "- 0. Basic tensor operations in TensorFlow\n",
    "\n",
    "\n",
    "- 1. From biological neurons to logic gates, to activation functions to universal function approximation (build your first ANN from scratch)\n",
    "\n",
    "\n",
    "- 2. Learning in ANNs: Gradient Descent, Backpropagation, and Automatic Differentiation (build your first ANN from scratch, including backpropagation and training loop)\n",
    "\n",
    "\n",
    "- 3. Basic usage of TensorFlow's automatic differentiation: The GradientTape context manager\n",
    "\n",
    "\n",
    "- 4. Modules, Layers, and Models. An introduction to the Keras Subclassing API\n",
    "\n",
    "\n",
    "- 5. Keras metrics for keeping track of losses, accuracies etc.\n",
    "\n",
    "\n",
    "- 6. Loss functions and optimizers\n",
    "\n",
    "\n",
    "- 7. Putting it together: Using TensorBoard to log training data and implementing a subclassed model using keras metrics and a custom training loop.\n",
    "\n",
    "\n",
    "- 8. Convolutional Neural Networks (incl. interactive widget)\n",
    "\n",
    "\n",
    "- 9. Regularization: Avoiding overfitting with L1/L2 penalties, dropout, normalization and data augmentation\n",
    "\n",
    "\n",
    "- 10. Optimization difficulties: Vanishing and exploding gradients. Weight initialization, normalization and residual/skip connections as partial solutions\n",
    "\n",
    "\n",
    "- 11. Recurrent Neural Networks: From unrolled recurrence to dynamically unrolled custom recurrent cells\n",
    "\n",
    "\n",
    "- (12. Autoencoders)\n",
    "\n",
    "\n",
    "- 13. Generative Models\n",
    "\n",
    "\n",
    "- (14. Transformers and NLP)\n",
    "\n",
    "\n",
    "- (15. Deep Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3379b",
   "metadata": {},
   "source": [
    "# Introduction: Implementing Artificial Neural Networks (ANNs) with TensorFlow\n",
    "\n",
    "The course starts with this first session (this notebook) on the basic notion of a neural network, their coarse similarities to biological neural networks. This session will introduce the notion of perceptrons, the general ideas of McCulloch and Pitts on expressing logical functions with networks of neurons. You will learn how to wire neurons into a network and set weights by hand to express a logical function. We will then go on to look at how a collection of such neurons can be expressed with the notion of a \"layer\" using matrix multiplication to perform the computation of multiple neurons in parallel. We will then connect a number of such layers to form a Multi-Layer-Perceptron, the simplest kind of ANN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1d260",
   "metadata": {},
   "source": [
    "## From biological neurons to ANNs\n",
    "\n",
    "Neurons can be abstracted to three core elements: a **cell body (soma)**, an **axon** and **dendrites**.\n",
    "\n",
    "Dendrites take in electrical input from other firing neurons. The cell body accumulates electrical potential until a **threshold** is reached and the neuron spikes an action potential, leading to an electric discharge via the axon, which will eventually reach the dendrites of other neurons.\n",
    "\n",
    "\n",
    "### Neuron and Computation\n",
    "\n",
    "How do neurons when reduced to these characteristics relate to computation? Already in 1943 [Warren McCulloch and Walter Pitts](https://link.springer.com/article/10.1007/BF02478259) studied the relation between abstract neurons and logical operators such as AND, OR and NOT. Since all of logic can be reduced to this minimal set of operators, combining neurons in a network allows to implement arbitrary logical computations based on inputs.\n",
    "\n",
    "A McCulloch-Pitts neuron outputs either 0 or 1 just as a logical operator/function would.\n",
    "\n",
    "\n",
    "## Logical functions\n",
    "\n",
    "Let's implement an arbitrary logical function that takes three boolean truth value arguments **a**, **b**, and **c** and evaluates the following logical expression:\n",
    "\n",
    "$$ \\neg a \\rightarrow (b \\land \\neg c) $$\n",
    "\n",
    "which is also equivalent to\n",
    "\n",
    "$$ a \\lor (b \\land \\neg c) $$\n",
    "\n",
    "Let's implement it in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8879d3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logical_function(arguments:tuple):\n",
    "    \n",
    "    # not(a) -> (b and not c)\n",
    "    \n",
    "    a,b,c = arguments\n",
    "    \n",
    "    if a:\n",
    "        return 1.\n",
    "    else:\n",
    "        if (b and not c):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "all_possible_arguments = \\\n",
    "            [(0,0,0),\n",
    "             (1,0,0),\n",
    "             (1,1,0),\n",
    "             (1,1,1),\n",
    "             (1,0,1),\n",
    "             (0,1,0),\n",
    "             (0,1,1),\n",
    "             (0,0,1)]\n",
    "\n",
    "logical_function(all_possible_arguments[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406cf333",
   "metadata": {},
   "source": [
    "Now we want to see that a **perceptron** can express this function.\n",
    "\n",
    "Before applying a threshold function (activation function), the output of a perceptron is given by\n",
    "\n",
    "$$ o = \\sum_i^m W_i x_i + b$$\n",
    "\n",
    "or, in matrix notation:\n",
    "\n",
    "$$ o = W x  + b$$\n",
    "\n",
    "Note that you often also see $o = x W + b$ - it all depends on whether the input vector x is a row or a column vector. \n",
    "\n",
    "On this scalar output, we apply the thresholding function to determine whether the perceptron unit is \"firing\" (it outputs 1) or not (it outputs 0). The following threshold function is known as the **heaviside-step function**:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\begin{cases}\n",
    "    1 ,& \\text{if } x > 0\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To relate this back to coarse biological inspiration, we can read the weights W as analogous to the synaptic strengths of dendritic connections, and the bias as shifting the threshold.\n",
    "\n",
    "This is all we need to know in order to define a perceptron in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e140c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def heaviside_step(x):\n",
    "    \"\"\"\n",
    "    Hard binary threshold function.\n",
    "    \n",
    "    A unit fires if a threshold of 0 is surpassed.\n",
    "    \"\"\"\n",
    "    return 1. if x > 0 else 0.\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, weight_values, bias, threshold_function=heaviside_step):\n",
    "        \n",
    "        self.weights = np.array([weight_values], dtype=np.float32)\n",
    "        self.bias = bias\n",
    "        self.threshold_function = threshold_function\n",
    "    \n",
    "    def __call__(self, x:np.ndarray):\n",
    "        \n",
    "        o = self.weights @ x + self.bias # where \"@\" is matrix multiplication (or dot product)\n",
    "        \n",
    "        return self.threshold_function(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532fe50",
   "metadata": {},
   "source": [
    "Can we configure the perceptron to express the logical function from above? The bias shifts the threshold for the weighted sum of the arguments at which the unit returns a 1. If the bias is very high, the threshold will be very low and the unit will always fire, regardless of the input. If the bias is a large negative number, the threshold will be high, meaning the weights and their input need to be large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1fba75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_weights = [2, 1, -1] # solution\n",
    "p_bias = 0\n",
    "\n",
    "perceptron = Perceptron(weight_values=p_weights, bias=p_bias)\n",
    "\n",
    "assert np.all(\n",
    "    [logical_function(x) == perceptron(np.array(x)) for x in all_possible_arguments]), \\\n",
    "\"the perceptron is not functionally equivalent ot the logical function\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0bf33e",
   "metadata": {},
   "source": [
    "## The logic of (binary) Multilayer Perceptrons (MLP)\n",
    "\n",
    "The previous logical expression could be modeled with a single perceptron. \n",
    "But what about the following logical function?\n",
    "\n",
    "$$ (a \\lor b) \\land \\neg(a \\land b)$$\n",
    "\n",
    "This is called the XOR operator for \"exclusive or\".\n",
    "\n",
    "It turns out that a single perceptron can not express XOR since it is not linearly separable. Perceptrons can only express a linear function.\n",
    "\n",
    "The **linear** separatrix (decision boundary) of a perceptron is given by\n",
    "$$ w_1x_1+w_2x_2+...+w_nx_n+b = 0$$\n",
    "\n",
    "![https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/Separability_YES.svg/1280px-Separability_YES.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/Separability_YES.svg/1280px-Separability_YES.svg.png)\n",
    "\n",
    "To express the non-linear function XOR, we can stack perceptrons on top of each other, where one perceptron can take the outputs of other perceptrons as its input arguments. This is exactly equivalent to how we can combine logical operators to create composite logical functions (such as XOR!).\n",
    "\n",
    "To see that XOR is not learnable with a single linear decision boundary, we can try to draw one, separating input for which a zero should be returned (red points) from input for which a one should be returned (blue points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0b4eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x=[1,0],y=[1,0], s=[200,200],c=\"red\")\n",
    "plt.scatter(x=[1,0,],y=[0,1], s=[200,200],c=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb77035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor(x):\n",
    "    a,b = x\n",
    "    return (a or b) and (not (a and b))\n",
    "\n",
    "all_possible_arguments = [\n",
    "    (0,0),\n",
    "    (0,1),\n",
    "    (1,0),\n",
    "    (1,1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a694fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a or b\n",
    "p1_weights = [1,1]\n",
    "p1_bias = 0\n",
    "\n",
    "# a and b\n",
    "p2_weights = [0.5,0.5]\n",
    "p2_bias = -0.5\n",
    "\n",
    "# p1 and not p2\n",
    "p3_weights = [0.5,-0.5]\n",
    "p3_bias = 0\n",
    "\n",
    "perceptron_1 = Perceptron(p1_weights, p1_bias)\n",
    "perceptron_2 = Perceptron(p2_weights, p2_bias)\n",
    "perceptron_3 = Perceptron(p3_weights, p3_bias)\n",
    "\n",
    "def wire_perceptrons(x):\n",
    "    \"\"\"\n",
    "    perceptron 1 and 2 take (a,b) as inputs.\n",
    "    perceptron 3 takes the outputs of perceptron 1 and perceptron 2 as inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    return perceptron_3([perceptron_1(x),perceptron_2(x)])\n",
    "\n",
    "# evaluate that our simple handwired network implements xor\n",
    "assert np.all([xor(x) == wire_perceptrons(np.array(x)) for x in all_possible_arguments]), \"Not equivalent to XOR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f380d0",
   "metadata": {},
   "source": [
    "We just implemented an artificial neural network that implements a non-linear logical function by setting all the weights by hand!\n",
    "\n",
    "What about other kinds of functions, like non-binary functions? So far, with the heaviside-step activation function, our perceptron can only output 1 or 0..\n",
    "\n",
    "So to get a continuous output, we need to use a continuously valued threshold function. One class of such activation functions is logistic functions: e.g. sigmoid and hyperbolic tangent:\n",
    "\n",
    "## Sigmoid activation function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- saturates towards 0 and 1\n",
    "- can thus be read as a probability\n",
    "\n",
    "## Tanh activation function\n",
    "\n",
    "$$ \\sigma(x) = \\frac{(e^x â e^{-x})}{(e^x + e^{-x})} $$\n",
    "\n",
    "- saturates at -1 and 1\n",
    "\n",
    "\n",
    "## No activation function (linear activation)\n",
    "\n",
    "$$ \\sigma(x) = x $$\n",
    "\n",
    "- can lead to arbitrary output values depending on what the pre-activation is\n",
    "- if the pre-activation is discrete valued, the output will also be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7157e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed9f7717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23147522])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron(weight_values=[0.5, -0.6, 0.1], bias=0, threshold_function=sigmoid)\n",
    "\n",
    "perceptron(np.array([-3,0,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84f6cd",
   "metadata": {},
   "source": [
    "## Layers of perceptrons\n",
    "\n",
    "As you will see, such perceptrons with non-linear activation functions present the basic units of artificial neural networks, however it is common to talk about layers of units and not single units.\n",
    "\n",
    "In the XOR example we had two perceptrons that took the raw values (a,b) as inputs. These can be considered to constitute a **layer** of two perceptrons. For readability and for matrix multiplication efficiency we want to implement a layer of perceptrons as a single object rather than having an object for each unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c34fd0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronLayer(object):\n",
    "    def __init__(self, weights, biases, activation_function=sigmoid):\n",
    "        \n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(biases)\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        output = (x @ self.weights + self.bias)\n",
    "        \n",
    "        return self.activation_function(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf512e2",
   "metadata": {},
   "source": [
    "You may notice that this looks exactly the same as before.\n",
    "\n",
    "This time, we pass a different shape of the weights array, turning the dot product (@) into a matrix-vector multiplication.\n",
    "\n",
    "Let's instantiate a layer of **4 perceptrons** that each connect to **3 input values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e937824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights have shape (3, 4)\n",
      "output: [0.99999992 0.07585818 1.         0.76852478]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([[1,3,4],\n",
    "                    [-2,-4,1],\n",
    "                    [3,1,9],\n",
    "                    [0,1,0]]).T\n",
    "\n",
    "biases = np.array([0,0,0,0])\n",
    "\n",
    "print(f\"weights have shape {weights.shape}\")\n",
    "\n",
    "perceptron_layer = PerceptronLayer(weights, biases)\n",
    "\n",
    "print(\"output:\", perceptron_layer([0.4, 1.2, 3.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78aff76",
   "metadata": {},
   "source": [
    "# Multi-Layer-Perceptrons\n",
    "\n",
    "\n",
    "From the layer with 4 units we get a 4 dimensional vector instead of a single scalar as the output. \n",
    "\n",
    "How can we stack these layers to construct an even more expressive neural network?\n",
    "\n",
    "All we need to do is to make sure we obey the rules of matrix multiplication!\n",
    "\n",
    "Matrix multiplication requires the inner dimensions of the two matrices match, e.g. (a,b) and (b,c).\n",
    "\n",
    "Given a matrix **W** (the weights of our layer) and a vector **x** (the inputs to the layer), we need the dimensionality of **x** to match the number of columns in the matrix. The output then is a vector with a dimensionality of the number of rows of **W**. The weights shape can be read as (n_inputs, n_outputs) and the input always has shape (batch_size, n_inputs), such that a matrix multiplication with **W** leads to shape (batch_size, n_outputs).\n",
    "\n",
    "If we have data x with shape (1,10) and a first layer with a weight shape of (10,5), we get a vector of shape (1,5) as the output. If we want to apply another layer to this output, its weights need to be of shape (5, n_units).\n",
    "\n",
    "Let's define such a network, this time using randomly initialized weights so we can also scale it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424a2acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [1.27902114]\n"
     ]
    }
   ],
   "source": [
    "def random_weights(*shape):\n",
    "    return np.random.normal(size=shape)\n",
    "\n",
    "class MultiLayerPerceptron(object):\n",
    "    \n",
    "    def __init__(self, n_inputs):\n",
    "        \n",
    "        self.layer_1 = PerceptronLayer(weights=random_weights(n_inputs, 64),\n",
    "                                       biases=np.zeros(64))\n",
    "        \n",
    "        self.layer_2 = PerceptronLayer(weights=random_weights(64, 32),\n",
    "                                       biases=np.zeros(32))\n",
    "        \n",
    "        self.layer_3 = PerceptronLayer(weights=random_weights(32, 16),\n",
    "                                       biases=np.zeros(16))\n",
    "        \n",
    "        self.output_layer = PerceptronLayer(weights=random_weights(16, 1),\n",
    "                                            biases=np.zeros(1),\n",
    "                                            activation_function=linear)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "    \n",
    "mlp = MultiLayerPerceptron(n_inputs=3)\n",
    "\n",
    "observation = [1, -1, 0.5]\n",
    "prediction = mlp(observation)\n",
    "print(\"output:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf7ea8",
   "metadata": {},
   "source": [
    "# Universal Function Approximation Theorem\n",
    "\n",
    "What can we do with this MLP architecture?\n",
    "\n",
    "According to the universal approximation theorems, given enough units in a single layer MLP with a non-linearity, we can approximate any continuous function. With multiple layers, we can usually achieve the same approximation with less units.\n",
    "\n",
    "MLPs present a powerful tool to learn from data, but it is not as simple as this finding may seem to suggest:\n",
    "\n",
    "- just because a solution exists for a network architecture, it does not mean we will find it (learning weights and biases is hard)\n",
    "- fitting to observed data does not mean the network will perform as well on unseen data  (generalization is hard)\n",
    "\n",
    "\n",
    "# How to update the weights of an MLP?\n",
    "\n",
    "Now suppose we have a set of observations (data) and we know the true values (labels) of what we want to predict.\n",
    "\n",
    "**How can we adapt the weights (and the bias values) of our multi-layer perceptron to make model predictions match the observed ground truth?**\n",
    "\n",
    "This will be addressed in detail in notebook 02."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
