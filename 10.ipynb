{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249729bd",
   "metadata": {},
   "source": [
    "# Optimization difficulties: Vanishing and exploding gradients\n",
    "\n",
    "In this session we explore the problem of vanishing or exploding gradients that frequently lead to optimization problems, especially in particularly deep (many layers) architectures.\n",
    "\n",
    "What is vanishing gradients? If you remember from session 02, backpropagation consists of a series of matrix multiplications of a layer's weights with the gradients of the loss function with respect to its output. This means that if the weights have a small magnitude, the gradients will be downscaled. If this happens sequentially, after a couple of layers, the gradients will be too small to lead to effective learning in earlier layers. This is especially important for recurrent neural networks as here the effective depth is not just the number of layers but the number of layers times the number of time-steps!\n",
    "\n",
    "On the contrary, if the weights are initialized with a variance that's too high, it can happen that gradients explode since backpropagating through each layer scales the gradients up, leading to unstable learning and possibly even numeric overflow.\n",
    "\n",
    "Ideally learning dynamics are somewhere between instable and too stable learning, sometimes referred to as the \"edge of chaos\" and we want to initialize weights accordingly. \n",
    "\n",
    "You may also remember from session 02 that we also multiply gradients by the derivative of the activation function. Now if we use sigmoid or tanh activation functions (saturating non-linearities in general), the gradients w.r.t. inputs when the output is towards saturation are diminishingly small. This can help with exploding gradient issues but it does not help with vanishing gradients. Partially linear activation functions such as the rectified linear unit activation function (ReLU) are better in this regard. ReLUs either multipy the gradient by 0 (if the input is negative) or by 1 (if the input is above 1).\n",
    "\n",
    "ML researchers have come up with a plethora of tools to avoid vanishing gradients or circumvent them to some degree. We will look at two options to deal with vanishing gradients:\n",
    "\n",
    "- smart scaling of the variance\n",
    "- partially linear activation functions \n",
    "- using skip connections in deep architectures, allowing for gradients to flow more directly from the output layer to the earlier layers\n",
    "- batch, layer, instance and group normalization, rescaling and off-setting the input in different ways such that gradients can flow better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05419b3",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "There are different schemes for initialization that are commonly used. The most often used initialization is **He-Uniform initialization** (we used this in the example of session 02), where the weights are sampled from a uniform distribution with min=-limit and max=limit, where limit = sqrt(6 / n_inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52749ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance of weights 0.19253153\n",
      "variance of weights 0.019861732\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tensor = tf.random.uniform((32,10))\n",
    "layer = tf.keras.layers.Dense(32,kernel_initializer=tf.keras.initializers.HeUniform())\n",
    "layer(tensor)\n",
    "print(\"variance of weights\", layer.trainable_variables[0].numpy().var())\n",
    "\n",
    "tensor = tf.random.uniform((32,100))\n",
    "layer = tf.keras.layers.Dense(128,kernel_initializer=tf.keras.initializers.HeUniform())\n",
    "layer(tensor)\n",
    "print(\"variance of weights\", layer.trainable_variables[0].numpy().var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8fff2a",
   "metadata": {},
   "source": [
    "**Glorot Uniform initilization** (this is the default for TensorFlow) is the same as He-Uniform but with limit = sqrt(6 / (n_inputs + n_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5255eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance of weights 0.04799015\n",
      "variance of weights 0.008802493\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.random.uniform((32,10))\n",
    "layer = tf.keras.layers.Dense(32,kernel_initializer=tf.keras.initializers.GlorotUniform())\n",
    "layer(tensor)\n",
    "print(\"variance of weights\", layer.trainable_variables[0].numpy().var())\n",
    "\n",
    "tensor = tf.random.uniform((32,100))\n",
    "layer = tf.keras.layers.Dense(128,kernel_initializer=tf.keras.initializers.GlorotUniform())\n",
    "layer(tensor)\n",
    "print(\"variance of weights\", layer.trainable_variables[0].numpy().var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b7985",
   "metadata": {},
   "source": [
    "# Residual and skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cf196b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient norm for the input 0.0202007275\r\n"
     ]
    }
   ],
   "source": [
    "# no skip or residual connections\n",
    "\n",
    "layer1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "layer2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "layer3 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "layer4 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(inputs)\n",
    "    layer1_out = layer1(inputs)\n",
    "    \n",
    "    layer2_out = layer2(layer1_out)\n",
    "    \n",
    "    layer3_out = layer3(layer2_out)\n",
    "    \n",
    "    layer4_out = layer4(layer3_out)\n",
    "    mean = tf.reduce_mean(layer4_out)\n",
    "tf.print(\"gradient norm for the input\", tf.norm(tape.gradient(mean, inputs),ord=\"euclidean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fc2e8f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient norm for the input 0.0324114449\r\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.random.uniform((32,128))\n",
    "\n",
    "layer1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "layer2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "layer3 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "layer4 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(inputs)\n",
    "    layer1_out = layer1(inputs)\n",
    "    layer2_out = layer2(layer1_out)\n",
    "    layer3_out = layer3(layer2_out+layer1_out) # residual connection\n",
    "    layer4_out = layer4(layer3_out+layer2_out) # residual connection\n",
    "    mean = tf.reduce_mean(layer4_out)\n",
    "tf.print(\"gradient norm for the input\", tf.norm(tape.gradient(mean, inputs),ord=\"euclidean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "08c1e700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient norm for the input 0.0306733567\r\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.random.uniform((32,128))\n",
    "\n",
    "layer1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "layer2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "layer3 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "layer4 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(inputs)\n",
    "    layer1_out = layer1(inputs)\n",
    "    \n",
    "    layer2_out = layer2(tf.concat([layer1_out, \n",
    "                                   inputs],axis=-1)) # skip connection\n",
    "    \n",
    "    layer3_out = layer3(tf.concat([layer2_out, \n",
    "                                   layer1_out, \n",
    "                                   inputs],axis=-1)) # skip connection\n",
    "    \n",
    "    layer4_out = layer4(tf.concat([layer3_out, \n",
    "                                   layer2_out, \n",
    "                                   layer1_out],axis=-1)) # skip connection\n",
    "    mean = tf.reduce_mean(layer4_out)\n",
    "tf.print(\"gradient norm for the input\", tf.norm(tape.gradient(mean, inputs),ord=\"euclidean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f13ad",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "**Batch normalization** uses statistics over the batch dimension to rescale and shift the inputs. It typically has different training and testing behavior which is why you need to set the training argument (like with dropout).\n",
    "\n",
    "1. Calculation of Mean and Variance:\n",
    "\n",
    "$$\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_i $$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu)^2$$\n",
    "\n",
    "2. Normalization:\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "3. Scaling and Shifting:\n",
    "\n",
    "$$y_i = \\gamma \\hat{x}_i + \\beta$$\n",
    "\n",
    "where:\n",
    "- $(X)$ is the mini-batch of activations in a layer,\n",
    "- $(m)$ is the number of examples in the mini-batch,\n",
    "- $(x_i)$ represents the (i)-th element in the mini-batch,\n",
    "- ($\\mu$) is the mean of the mini-batch,\n",
    "- ($\\sigma^2$) is the variance of the mini-batch,\n",
    "- ($\\epsilon$) is a small value for numerical stability,\n",
    "- ($\\hat{x}_i$) is the normalized value of ($x_i$),\n",
    "- ($\\gamma$) and ($\\beta$) are learnable parameters for scaling and shifting the normalized values,\n",
    "- ($y_i$) is the final output after normalization, scaling, and shifting.\n",
    "\n",
    "**Layer Normalization** does not compute any statistics over the batch dimension and thus also works with smaller batch sizes. However layer normalization does have more parameters than batch normalization because it maintains separate learnable affine parameters $\\gamma$ and $\\beta$ for the different input values.\n",
    "\n",
    "1. Calculation of Mean and Variance:\n",
    "\n",
    "$$\\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2$$\n",
    "\n",
    "where:\n",
    "- $(H)$ is the number of units/neurons in the layer (for the computation of the mean and variance, the normalization is done over the same layer's units).\n",
    "\n",
    "2. Normalization:\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "3. Scaling and Shifting:\n",
    "$$y_i = \\gamma_i \\hat{x}_i + \\beta_i$$\n",
    "\n",
    "where:\n",
    "$(x_i)$ represents the $(i)$-th element in the layer's units,\n",
    "- $(\\mu)$ is the mean of the layer's units,\n",
    "- $(\\sigma^2)$ is the variance of the layer's units,\n",
    "- $(\\epsilon)$ is a small value for numerical stability,\n",
    "- $(\\hat{x}_i)$ is the normalized value of $(x_i)$,\n",
    "- $(\\gamma)$ and $(\\beta)$ are learnable parameters for scaling and shifting the normalized values,\n",
    "- $(y_i)$ is the final output after normalization, scaling, and shifting.\n",
    "\n",
    "The key difference between Batch Normalization and Layer Normalization is that Batch Normalization normalizes the input using the statistics from a mini-batch of data, whereas Layer Normalization normalizes the input using statistics calculated over the same layer's units (i.e., across the feature dimension). Layer Normalization is commonly used in recurrent neural networks (RNNs) where the mini-batch size may vary significantly, making Batch Normalization less suitable.\n",
    "\n",
    "\n",
    "**Input normalization** makes sense to use to help the network center its activations around zero at initialization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
